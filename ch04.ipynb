{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca103c8b",
   "metadata": {},
   "source": [
    "# 4章 word2vecの高速化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46dc287",
   "metadata": {},
   "source": [
    "本章では，word2vecの高速化に主眼を置き，word2vecの改善に取り組む．\n",
    "\n",
    "#### 2つの改良を加えて，本物のword2vecを作成する\n",
    "- Embeddingレイヤという新しいレイヤを導入する\n",
    "- Negative Samplingという新しい損失関数を導入する\n",
    "\n",
    "\n",
    "最終的には，PTBデータセット（実用的なサイズのコーパス）を対象に学習を行う．\n",
    "\n",
    "→単語の分散表現の良さを実際に評価する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec27dd2",
   "metadata": {},
   "source": [
    "## 4.1 word2vecの改良①"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ca46a",
   "metadata": {},
   "source": [
    "### 4.1.1 Embeddintレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071cc1e",
   "metadata": {},
   "source": [
    "仮に100万語の語彙数からなるコーパスがあれば，単語のone-hot表現の次元数は100万になる．そして巨大なベクトルと重み行列の積を計算する必要があった．しかし，ここで行っているのことは，行列の特定の行を抜き出すことだけ，なのでone-hot表現への変換と，MatMulレイヤでの行列の乗算は必要なさそう．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e0d1d",
   "metadata": {},
   "source": [
    "### 4.1.2 Embeddingレイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05451198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]\n",
      " [12 13 14]\n",
      " [15 16 17]\n",
      " [18 19 20]]\n",
      "[6 7 8]\n",
      "[15 16 17]\n"
     ]
    }
   ],
   "source": [
    "# 重みから特定の行を抜き出すには．．．\n",
    "import numpy as np\n",
    "W = np.arange(21).reshape(7, 3)\n",
    "print(W)\n",
    "print(W[2])\n",
    "print(W[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15df58e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  4  5]\n",
      " [ 0  1  2]\n",
      " [ 9 10 11]\n",
      " [ 0  1  2]]\n"
     ]
    }
   ],
   "source": [
    "# 複数の行を抜き出す  ミニバッチを想定した実装\n",
    "idx = np.array([1, 0, 3, 0])\n",
    "print(W[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8842fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]\n",
      " [12 13 14]\n",
      " [15 16 17]\n",
      " [18 19 20]]\n",
      "[array([[ 0,  1,  2],\n",
      "       [ 3,  4,  5],\n",
      "       [ 6,  7,  8],\n",
      "       [ 9, 10, 11],\n",
      "       [12, 13, 14],\n",
      "       [15, 16, 17],\n",
      "       [18, 19, 20]])]\n",
      "[array([[0, 0, 0],\n",
      "       [0, 0, 0],\n",
      "       [0, 0, 0],\n",
      "       [0, 0, 0],\n",
      "       [0, 0, 0],\n",
      "       [0, 0, 0],\n",
      "       [0, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.arange(21)\n",
    "print(a)\n",
    "a = a.reshape(7, 3)\n",
    "print(a)\n",
    "\n",
    "# embedding層の実装\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        print(dW)\n",
    "        dW[...] = 0\n",
    "        \n",
    "        for i, word_id in enumerate(self.idx):\n",
    "            dW[word_id] += dout[i]\n",
    "        # もしくは\n",
    "        # np.add.at(dW, self.idx, dout)\n",
    "        \n",
    "        return None\n",
    "a = Embedding(a)\n",
    "\n",
    "a.forward(np.array([0, 1, 2]))\n",
    "\n",
    "print(a.params)\n",
    "print(a.grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4290293",
   "metadata": {},
   "source": [
    "Embeddingレイヤの順伝播は，重みWの特定の行を抜き出すだけ，つまり重みの特定の行のニューロンだけを何の手も加えず次の層へと流したことになる．逆伝播の場合は，前の層から伝わってきた勾配を次の層へそのまま流すだけ．しかし，前層から伝わる勾配を，重みの勾配dWの特定の行（idx）に設定するようにする．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a45ee1a",
   "metadata": {},
   "source": [
    "まとめると，入力側のMatMulレイヤをEmbeddingレイヤに切り替えて無駄な計算を省いたという話．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4178c7b5",
   "metadata": {},
   "source": [
    "## 4.2 word2vecの改良② "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992e646",
   "metadata": {},
   "source": [
    "残るボトルネックは，中間層以降の処理，行列の積とSoftmaxレイヤの計算である．\n",
    "#### Negative Sampling(負例サンプリング)\n",
    "Softmaxの代わりにNegative Samplingを用いることで，語彙数がどれだけ多くなったとしても，計算量を少なく一定に抑えることができる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283b4217",
   "metadata": {},
   "source": [
    "### 4.2.1 中間層以降の計算の問題点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f6d62b",
   "metadata": {},
   "source": [
    "前節に引き続き，語彙数が100万，中間層のニューロン数が100のときのword2vec(CBOWモデル)を考える．\n",
    "\n",
    "入力層と出力層には100万個のニューロンが存在する．入力層の計算についてはEmbeddingレイヤを導入することで，無駄を省く方法を見てきた．残る問題としては中間層以降の処理だが．中間層に関しては以下の2つの場所で多くの計算時間が必要になる．\n",
    "\n",
    "- 中間層のニューロンと重み行列($W_out$)の積\n",
    "- Softmaxレイヤの計算\n",
    "\n",
    "巨大な行列の積は，多くの計算時間と多くのメモリを必要とする．この処理を軽くする必要がある．\n",
    "\n",
    "#### Softmaxの計算量の大きさを式で確認する\n",
    "### $$\n",
    "y_k = \\frac{exp(s_k)}{\\sum_{i=0}^{1000000}exp(s_i) \\quad}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6b9070",
   "metadata": {},
   "source": [
    "### 4.2.2 多値分類から二値分類へ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e4a7f",
   "metadata": {},
   "source": [
    "#### Negative samplingを理解する上で重要なポイント\n",
    "「多値分類」を「二値分類」で近似すること．\n",
    "\n",
    "ターゲットとなる単語だけのスコアを求めるニューラルネットワークを構築し，ターゲットの単語が答えであるか，否かという二値分類問題を近似的に作り出す．\n",
    "\n",
    "上記を考慮すると中間層と出力側の重みの行列の積は，ターゲットに対応する列（単語ベクトル）だけを抽出し，その抽出したベクトルと中間層のニューロンとの内積を計算すればよいことになる．そしてこれが最終的なスコアになる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce112a84",
   "metadata": {},
   "source": [
    "### 4.2.3 シグモイド関数と交差エントロピー誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3916f22",
   "metadata": {},
   "source": [
    "#### 多値分類\n",
    "出力層には「ソフトマックス関数」，損失関数には「交差エントロピー誤差」を用いる．\n",
    "#### 二値分類\n",
    "出力層には「シグモイド関数」，損失関数には「交差エントロピー誤差」を用いる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce46b214",
   "metadata": {},
   "source": [
    "#### シグモイド関数\n",
    "$$ \n",
    "y = \\frac{1}{1+exp(-x)}\n",
    "$$\n",
    "\n",
    "#### 交差エントロピー誤差\n",
    "$$\n",
    "L = -(tlogy+(1-t)log(1-y))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e50d60",
   "metadata": {},
   "source": [
    "#### 逆伝播時に伝播するy - t\n",
    "- y: ニューラルネットワークが出力した確率\n",
    "- t: 正解ラベル\n",
    "\n",
    "つまりy - tはその2つの誤差．誤差が前レイヤに流れたとき，大きい誤差の場合は大きく学習し，小さい誤差の場合は小さく学習するようになる．\n",
    "\n",
    "#### y - tというきれいな結果\n",
    "- シグモイド関数と交差エントロピー誤差\n",
    "- ソフトマックス関数と交差エントロピー誤差\n",
    "- 恒等関数と2乗和誤差\n",
    "\n",
    "上の3つの組み合わせはどれも逆伝播時にはy - tの値が伝播する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc27164a",
   "metadata": {},
   "source": [
    "### 4.2.4 多値分類から二値分類へ（実装編）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d854e9",
   "metadata": {},
   "source": [
    "#### Embedding Dotレイヤ\n",
    "Embeddingレイヤとdot演算(内積)の2つの処理を合わせたレイヤ．中間層以降の処理だけにフォーカスして描画する．EmbeddingDotレイヤを使って，Embeddingレイヤと内積の計算をまとめて行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98703e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EmbiddingDotレイヤ\n",
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "        \n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "        \n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe81f575",
   "metadata": {},
   "source": [
    "### 4.2.5 Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd239b",
   "metadata": {},
   "source": [
    "本当に行いたいことは，正例についてはSigmoidレイヤの出力を1に近づけ，負例については，Sigmoidレイヤの出力を0に近づけること．\n",
    "\n",
    "コンテキストに対して誤ったターゲットである場合の確率は低い値であることが望まれる．\n",
    "\n",
    "#### 多値分類の問題を二値分類として扱うためには，「正しい答え（正例）」と「間違った答え（負例）」のそれぞれに対して，正しく分類できる必要がある．そのため，正例と負例の両者を対象として問題を考える必要がある．\n",
    "\n",
    "#### Negative Sampling\n",
    "すべての負例を対象にして，二値分類の学習を行うことはしない．その代わりに近似解としていくつかピックアップする．つまりネガティブな例を少数サンプリングする．損失関数は，正例をターゲットとした場合の損失といくつかサンプリングした負例の損失を足し合わせたものになる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee02978",
   "metadata": {},
   "source": [
    "### 4.2.6 Negative Samplingのサンプリング方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136e8f1",
   "metadata": {},
   "source": [
    "#### 負例のサンプリング\n",
    "ランダムにサンプリングするよりも良い方法が知られている．それはコーパスの統計データに基づいたサンプリングを行うこと．コーパス中でよく使われる単語は抽出されやすくし，コーパスの中であまり使われない単語は抽出されにくくする．\n",
    "\n",
    "#### word2vecで提案されたNegative samplingにおける確率分布の式\n",
    "### $$\n",
    "P^{'}(W_i) = \\frac{P(w_i^{0.75})}{\\sum_{j}^{n}P(w_j)^{0.75}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253da4c1",
   "metadata": {},
   "source": [
    "#### UnigramSamplerクラス\n",
    "実装内容に興味がある場合には各自参照．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "190cbf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 3]\n",
      " [4 0]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "# UnigramSamplerクラスを使って処理してみる\n",
    "\n",
    "from negative_sampling_layer import UnigramSampler\n",
    "\n",
    "# 使用する引数は3つ\n",
    "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
    "power = 0.75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1, 3, 0])\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36488cc",
   "metadata": {},
   "source": [
    "### 4.2.7 Negative Samplingの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e932d",
   "metadata": {},
   "source": [
    "最後にNegativeSamplingの実装を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbdc5be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Sampling\n",
    "\n",
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        # 負例samplesize+正例1つ分のレイヤをそれぞれリストで保持する\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size \\\n",
    "                                                            +1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size \\\n",
    "                                                               + 1)]\n",
    "        self.params, self.grads = [], []\n",
    "        # このレイヤで使用する配列をそれぞれ配列にまとめる．\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "        print(self.params, self.grads)\n",
    "        \n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "        \n",
    "        # 正例のフォワード\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "        \n",
    "        # 負例のフォワード\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    # 逆伝播\n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backword(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "            \n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1793e893",
   "metadata": {},
   "source": [
    "## 4.3 改良版word2vecの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9a718",
   "metadata": {},
   "source": [
    "- Embeddingレイヤ\n",
    "- Negative Sampling\n",
    "\n",
    "2つの手法を取り入れて，ニューラルネットワークを作成する．そしてPTBデータセットを使って学習し，より実用的な単語の分散表現を獲得する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02554ae4",
   "metadata": {},
   "source": [
    "### 4.3.1 CBOWモデルの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "828ec19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOWクラス\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import Embedding\n",
    "from negative_sampling_layer import NegativeSamplingLoss\n",
    "\n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        \n",
    "        # 重みの初期化\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, \\\n",
    "                                           sample_size=5)\n",
    "        \n",
    "        # すべての重みと勾配を配列にまとめる\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "        # メンバ変数に単語の分散表現を設定\n",
    "        self.word_vecs = W_in\n",
    "        \n",
    "    # 順伝播\n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "    \n",
    "    # 逆伝播\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4179710",
   "metadata": {},
   "source": [
    "### 4.3.2 CBOWモデルの学習コード"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6673f97f",
   "metadata": {},
   "source": [
    "最後に，CBOWモデルの学習を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f0a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ニューラルネットワークの学習を行うだけ\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('..')\n",
    "# import numpy as np\n",
    "# from common import config\n",
    "# import pickle\n",
    "# from common.trainer import Trainer\n",
    "# from common.optimizer import Adam\n",
    "# from cbow import CBOW\n",
    "# from common.util import create_contexts_target, to_cpu, to_gpu\n",
    "# from dataset import ptb\n",
    "\n",
    "# # ハイパーパラメータの設定\n",
    "# window_size = 5\n",
    "# hidden_size = 100\n",
    "# batch_size = 100\n",
    "# max_epoch = 10\n",
    "\n",
    "# # データの読み込み\n",
    "# corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "# vocab_size = len(word_to_id)\n",
    "\n",
    "# contexts, target = create_contexts_target(corpus, window_size)\n",
    "\n",
    "# # モデルなどの生成\n",
    "# model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "# optimizer = Adam()\n",
    "# trainer = Trainer(model, optimizer)\n",
    "\n",
    "# # 学習開始\n",
    "# trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "# trainer.plot()\n",
    "\n",
    "# # 後ほど利用できるように，必要なデータを保存\n",
    "# word_vecs = model.word_vecs\n",
    "\n",
    "# params = {}\n",
    "# params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "# params['word_to_id'] = word_to_id\n",
    "# params['id_to_word'] = id_to_word\n",
    "# pkl_file = 'cbow_params.pkl'\n",
    "# with open(self_pkl_file, 'wb') as f:\n",
    "#     pickle.dump(params, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9fc7c6",
   "metadata": {},
   "source": [
    "### 4.3.3 CBOWモデルの評価"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c91ba8",
   "metadata": {},
   "source": [
    "前節で学習した単語の分散表現を評価してみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3582c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_similar()メソッドを使用していくつかの単語に対して最も距離の近い単語\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from commmon.util import most_similar\n",
    "import pickle\n",
    "\n",
    "pkl_file = 'cbow_params.pkl'\n",
    "\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    word_vecs = params['word_vecs']\n",
    "    word_to_id = params['word_to_id']\n",
    "    id_to_word = params['id_to_word']\n",
    "    \n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93c04c",
   "metadata": {},
   "source": [
    "## 4.4 word2vecに関する残りのテーマ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1898d6f3",
   "metadata": {},
   "source": [
    "ここまでで，word2vecの仕組みや実装についての説明は終わり．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13d820",
   "metadata": {},
   "source": [
    "### 4.4.1 word2vecを使ったアプリケーションの例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b89b21",
   "metadata": {},
   "source": [
    "#### 転移学習\n",
    "単語の分散表現を他の分野のタスクに適用させる．テキスト分類や文書クラスタリング，品詞タグ付け，感情分析などの自然言語タスクにおいて，単語をベクトルに変換する最初のステップでは，学習済みの単語の分散表現を利用できる．単語の分散表現はそれらのタスクにおいて素晴らしい結果をもたらす．単語の分散表現については最初にwikiやgooglenewsなどの大きなコーパスで学習を行っておく．\n",
    "\n",
    "さらに単語の分散表現の利点として，単語を固定長のベクトルに変換できることにある．これは文章に対しても行える．自然言語をベクトルに変換できれば，一般的な機械学習の手法が適用できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f3343b",
   "metadata": {},
   "source": [
    "### 4.4.2 単語ベクトルの評価方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d280d5d3",
   "metadata": {},
   "source": [
    "#### 単語の分散表現の良さをどのように評価するか？？\n",
    "単語の分散表現の良さを評価するにあたり，現実的なアプリケーションとは切り離して評価を行う．よく用いられる評価手法は，「類似性」や「類推問題」．\n",
    "#### 単語の類似性の評価\n",
    "人間が作成した単語類似度の評価セットを使って評価する．\n",
    "#### 類推問題による単語ベクトルの評価結果からわかること\n",
    "- モデルによって精度が異なる（コーパスに応じて最適なモデルを選ぶ）\n",
    "- コーパスが大きいほど良い結果になる（ビックデータは常に望まれる）\n",
    "- 単語ベクトルの次元数は適度な大きさが必要（大きすぎても精度が悪くなる）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27723987",
   "metadata": {},
   "source": [
    "## 4.5 まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc159daf",
   "metadata": {},
   "source": [
    "#### 本章で学んだこと\n",
    "- Embeddingレイヤは単語の分散表現を格納し，順伝播において該当する単語IDベクトルを抽出する\n",
    "- word2vecでは語彙数の増加に比例して計算量が増加するので，近似計算を行う高速な手法を使うといい\n",
    "- NagativeSamplingは負例をいくつかサンプリングする手法であり，これを利用すれば多値分類を二値分類として扱うことができる\n",
    "- word2vecによって得られた単語の分散表現は，単語の意味が埋め込まれたものであり，似たコンテキストで使われる単語は単語ベクトルの空間上で近い場所に位置するようになる\n",
    "- word2vecの単語の分散表現は，類推問題をベクトルの加算と減算によって解ける性質を持つ\n",
    "- word2vecは転移学習の点で特に重要であり，その単語の分散表現はさまざまな自然言語処理のタスクに利用できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc77e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
