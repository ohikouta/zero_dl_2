{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e862dd",
   "metadata": {},
   "source": [
    "# 2章 自然言語と単語の分散表現"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4199af",
   "metadata": {},
   "source": [
    "この章では，コンピュータに言葉を理解させるということがどういうことなのか，どのようなアプローチが存在するのかを考察する．特に，古典的な手法を考え，ディープラーニング（ニューラルネットワーク）をベースとした手法は次章以降に考える．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca71a5",
   "metadata": {},
   "source": [
    "## 2.1 自然言語処理とは"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbb4be",
   "metadata": {},
   "source": [
    "#### 自然言語処理（NLP）の目標\n",
    "人の話す言葉をコンピュータに理解させ，私たちにとって役立つことをコンピュータに行わせること．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e84baf",
   "metadata": {},
   "source": [
    "### 2.1.1 単語の意味"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef5212",
   "metadata": {},
   "source": [
    "#### 本章のテーマ\n",
    "私たちの言葉は単語によって構成されるので，コンピュータが単語の意味を理解することは重要になる．\n",
    "- シソーラスによる手法\n",
    "- カウントベースの手法\n",
    "- 推論ベースの手法（word2vec）\n",
    "\n",
    "#### それぞれの概要\n",
    "- シソーラス：人の手によってつくられた類似辞書\n",
    "- カウントベース：統計情報から単語を表現する手法\n",
    "- 推論ベース：ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d04852",
   "metadata": {},
   "source": [
    "## 2.2 シソーラス"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587eaf44",
   "metadata": {},
   "source": [
    "シソーラスは基本的には，類似辞書であり，「同じ意味の単語（同義語）や「意味の似た単語（類義語）」が同じグループに分類されている．また，自然言語処理において利用されるシソーラスでは，単語の間で「上位と下位」，「全体と部分」などの，より細かい関連性が定義されている場合がある．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2342223",
   "metadata": {},
   "source": [
    "### 2.2.1 WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3542ae1b",
   "metadata": {},
   "source": [
    "#### WordNet\n",
    "自然言語処理の分野において，最も有名なシソーラス．類義語を取得したり，単語ネットワークを利用したりできる．単語ネットワークを利用して単語間の類似度を算出することも可能．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c716be",
   "metadata": {},
   "source": [
    "### 2.2.2 シソーラスの問題点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae759bf",
   "metadata": {},
   "source": [
    "WordNetのようなシソーラスは，多くの単語に対して同義語や階層構造などの関係性が定義されている．しかし人の手によるラベル付けには大きな欠点がいくつか存在する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccbfc8c",
   "metadata": {},
   "source": [
    "#### 時代の変化に対応するのが困難\n",
    "→単語の変化に対応するためにシソーラスを人手によって絶えず更新する必要がある．\n",
    "#### 人の作業コストが高い\n",
    "→膨大な単語に対して，単語の関連付けを行うのは非常に大変な作業になる．\n",
    "#### 単語の細かなニュアンスを表現できない\n",
    "→シソーラスは，類義語として似たような単語をグループ化するが，その中の単語はそれぞれに異なるニュアンスを持つ．例えば，ヴィンテージやレトロは同じような意味を表すが，使われ方は異なる，など．\n",
    "\n",
    "- カウントベースの手法\n",
    "- ニューラルネットワークを使った推論ベースの手法\n",
    "\n",
    "以上2つは，大量のテキストデータから自動的に単語の意味を抽出する．これはすなわち，人の手作業によって単語を関連付けるという重労働からの解放を意味する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f524883",
   "metadata": {},
   "source": [
    "## 2.3 カウントベースの手法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcaa8bb",
   "metadata": {},
   "source": [
    "カウントベースの手法を進む際にはコーパスを利用する．\n",
    "\n",
    "#### コーパス\n",
    "自然言語処理の研究やアプリケーションのために目的をもって収集されたデータのこと．コーパスは人の手によって書かれたものであり，コーパスには自然言語に対する人の知識がふんだんに含まれている．\n",
    "\n",
    "#### カウントベースの手法の目標\n",
    "人の知識が詰まったコーパスから，自動的に，効率よく，エッセンスを抽出すること．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9170b",
   "metadata": {},
   "source": [
    "### 2.3.1 Pythonによるコーパスの下準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "befecfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you say goodbye and i say hello .\n",
      "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# まずは小さなテキストデータから\n",
    "text = 'You say goodbye and I say hello.'\n",
    "\n",
    "# 小文字に統一してカンマを一つの単語にから独立させる\n",
    "text = text.lower()\n",
    "text = text.replace('.', ' .')\n",
    "print(text)\n",
    "\n",
    "# 単語分割\n",
    "words = text.split(' ')\n",
    "print(words)\n",
    "print(type(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f251827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語のIDと単語の対応表をPythonのディクショナリで作成\n",
    "\n",
    "word_to_id = {}  # \"word\": xx\n",
    "id_to_word = {}  # xx: \"word\"\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)  # その時点のword_to_idの長さ(例えば最初は0)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22dd6b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n"
     ]
    }
   ],
   "source": [
    "# 中身の確認\n",
    "print(id_to_word)\n",
    "print(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ebb7107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# 単語から単語IDの検索，単語IDから単語の検索が可能に\n",
    "print(id_to_word[1])\n",
    "print(word_to_id['hello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee43196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n"
     ]
    }
   ],
   "source": [
    "# 単語リストからNumPy配列の単語IDリストに変換\n",
    "import numpy as np\n",
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73bcf899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ただのテキストからコーパス作成までの処理を関数にまとめる\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "            \n",
    "    \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f71069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6] {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6} {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "# 関数を使ってコーパスの前処理を行う\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "print(corpus, word_to_id, id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65c6fae",
   "metadata": {},
   "source": [
    "これからの目標は，コーパスを使って「単語の意味」を抽出すること．今後はカウントベースの手法を見ていき，この手法によって単語をベクトルで表すことができるようになる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcea56d",
   "metadata": {},
   "source": [
    "### 2.3.2 単語の分散表現"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb9f8a",
   "metadata": {},
   "source": [
    "#### 単語の分散表現\n",
    "単語の意味を的確にとらえたベクトル表現"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286d59f",
   "metadata": {},
   "source": [
    "### 2.3.3 分布仮説"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d36fae6",
   "metadata": {},
   "source": [
    "#### 分布仮説\n",
    "単語の意味は，周囲の単語によって形成されるという考え．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36220ad4",
   "metadata": {},
   "source": [
    "#### コンテキスト\n",
    "文脈．ここではその周囲に存在する単語を指す．(window size: コンテキストのサイズ，周囲の単語をどれだけ含めるかを指定するもの)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15732fba",
   "metadata": {},
   "source": [
    "### 2.3.4 共起行列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b7167",
   "metadata": {},
   "source": [
    "#### 共起行列\n",
    "共起する単語をテーブルにまとめたもの．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bc6a910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "# カウントベース手法の下準備\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import preprocess\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "print(corpus)\n",
    "\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05797138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起行列を手打ちで作成する\n",
    "\n",
    "C = np.array([\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0],\n",
    "], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b15e7af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "[0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 各単語のベクトル\n",
    "print(C[0])\n",
    "print(C[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e952c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# コーパスから共起行列を作成する関数\n",
    "\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            \n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "                \n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8de616",
   "metadata": {},
   "source": [
    "この関数を使用することで，コーパスがどれだけ大きくなったとしても，自動で共起行列を作ることができる．今後はこの関数を使用して，コーパスの共起行列を作成する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979dd948",
   "metadata": {},
   "source": [
    "### 2.3.5 ベクトル間の類似度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70374051",
   "metadata": {},
   "source": [
    "共起行列によって単語をベクトルで表現することができた．次にベクトル間の類似度を計測する方法を考える．\n",
    "\n",
    "ベクトル間の類似度を計算する方法は様々な方法があるが，ベクトルの内積やユークリッド距離などが代表例として挙げられる．中でも単語のベクトル表現の類似度に関しては，コサイン類似度がよく用いられる．\n",
    "\n",
    "#### コサイン類似度\n",
    "直観的なイメージとしては，2つのベクトルがどれだけ同じ方向を向いているかを表す．2つのベクトルが完全に同じ方向を向いているときはコサイン類似度は1になり，完全に逆向きだとコサイン類似度は―1になる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "923281c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# コサイン類似度の実装\n",
    "\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x**2)) + eps)  # xの正規化\n",
    "    ny = y / (np.sqrt(np.sum(y**2)) + eps)  # yの正規化\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d77273fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067691154799\n"
     ]
    }
   ],
   "source": [
    "# youとiの類似度を計算してみる\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "from common.util import preprocess, create_co_matrix, cos_similarity\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['you']]  # [you]の単語ベクトル\n",
    "c1 = C[word_to_id['i']]    # [i]の単語ベクトル\n",
    "print(cos_similarity(c0, c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d0cf2",
   "metadata": {},
   "source": [
    "### 2.3.6 類似単語のランキング表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b981a0",
   "metadata": {},
   "source": [
    "ある単語がクエリとして与えられたときに，そのクエリに対して類似した単語を上位から順に表示する関数．（most_similar）\n",
    "\n",
    "- クエリ（単語）\n",
    "- 単語から単語IDへのディクショナリ \n",
    "- 単語IDから単語へのディクショナリ\n",
    "- 単語ベクトルをまとめた行列，各行に対応する単語のベクトルが格納されていることを想定する\n",
    "- 上位何位まで表示するか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86cc335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_similar\n",
    "\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    \n",
    "    # ①クエリを取り出す\n",
    "    if query not in word_to_id:\n",
    "        print('%s is not found' % query)\n",
    "        return \n",
    "    \n",
    "    print('\\n[query]' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "    \n",
    "    # ②コサイン類似度の算出\n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "        \n",
    "    # ③コサイン類似度の結果から，その値を高い順に出力\n",
    "    count = 0\n",
    "    for i in (-1*similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "        \n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1af2445a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067691154799\n",
      " i: 0.7071067691154799\n",
      " hello: 0.7071067691154799\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "# youをクエリとして類似する単語を表示してみる\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import preprocess, create_co_matrix, most_similar\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b582b",
   "metadata": {},
   "source": [
    "## 2.4 カウントベースの手法の改善"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5e2e59",
   "metadata": {},
   "source": [
    "### 2.4.1 相互情報量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13ebe87",
   "metadata": {},
   "source": [
    "#### 相互情報量（PMI）\n",
    "単純な回数だけで関連性を考慮するのではなく，単語独自の出現回数を考慮した式を用いたスコアづけを行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bebfd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起行列をPPMI行列に変換する関数を実装する\n",
    "def ppmi(C, verbose=False, eps=1e-8):\n",
    "    M = np.zers_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "            \n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100 + 1) == 0:\n",
    "                    print('%.1f%% done' % (100*cnt/total))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0b3d612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "# 実際に共起行列をPPMI行列に変換する\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import preprocess, create_co_matrix, cos_similarity, ppmi\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision=3)  # 有効桁3桁で表示\n",
    "print('covariance matrix')\n",
    "print(C)\n",
    "print('-' * 50)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e9a0f",
   "metadata": {},
   "source": [
    "### 2.4.2 次元削減"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a27fef3",
   "metadata": {},
   "source": [
    "#### 次元削減\n",
    "ベクトルの次元を削減する．単に削減するのではなく，重要な情報をできるだけ残した上で削減する．データの分布をみて，重要な軸を見つける．\n",
    "\n",
    "#### 疎なベクトル\n",
    "ベクトルの中のほとんどの要素が0である行列のこと．密なベクトルこそが求めたい単語の分散表現である．\n",
    "\n",
    "#### 特異値分解(Singular Value Decomposition:SVD)\n",
    "任意の行列を3つの行列の積に変換する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99e330",
   "metadata": {},
   "source": [
    "### 2.4.3 SVDによる次元削減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9675a111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Duplicate key in file WindowsPath('C:/Users/81804/anaconda3/lib/site-packages/matplotlib/mpl-data/matplotlibrc'), line 258 ('font.family:  sans-serif')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "[-3.409e-01 -1.110e-16 -3.886e-16 -1.205e-01  0.000e+00  9.323e-01\n",
      "  2.226e-16]\n"
     ]
    }
   ],
   "source": [
    "# SVDの実装\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from common.util import preprocess, create_co_matrix, ppmi\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "courpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocav_size = len(id_to_word)\n",
    "C = create_co_matrix(corpus, vocab_size, window_size=1)\n",
    "W = ppmi(C)\n",
    "\n",
    "# SVD\n",
    "U, S, V = np.linalg.svd(W)\n",
    "\n",
    "print(C[0])\n",
    "print(W[0])\n",
    "print(U[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f15981ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['IPAexGothic'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 各単語を2次元のベクトルで表し，それをグラフにプロットする．\n",
    "\n",
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "    \n",
    "plt.scatter(U[:, 0], U[:, 1], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f4c33",
   "metadata": {},
   "source": [
    "### 2.4.4 PTBデータセット"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59f8f9",
   "metadata": {},
   "source": [
    "#### PennTreebank(PTB)\n",
    "提案手法の品質を測定するためのベンチマークとしてよく利用される．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82afed81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 929589\n",
      "corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "id_to_word[0]: aer\n",
      "id_to_word[1]: banknote\n",
      "id_to_word[2]: berlitz\n",
      "\n",
      "word_to_id['car']: 3856\n",
      "word_to_id['happy']: 4428\n",
      "word_to_id['lexus']: 7426\n"
     ]
    }
   ],
   "source": [
    "# PTBデータセットの内容を少し確認してみる\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "\n",
    "print('corpus size:', len(corpus))\n",
    "print('corpus[:30]:', corpus[:30])\n",
    "print()\n",
    "print('id_to_word[0]:', id_to_word[0])\n",
    "print('id_to_word[1]:', id_to_word[1])\n",
    "print('id_to_word[2]:', id_to_word[2])\n",
    "print()\n",
    "print(\"word_to_id['car']:\", word_to_id['car'])\n",
    "print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
    "print(\"word_to_id['lexus']:\", word_to_id['lexus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7646fdd",
   "metadata": {},
   "source": [
    "### 2.4.5 PTBデータセットでの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99824b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting co-occurrence ...\n",
      "calculating PPMI ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\81804\\working_directory\\zero_dl_2\\common\\util.py:139: RuntimeWarning: overflow encountered in long_scalars\n",
      "  pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
      "C:\\Users\\81804\\working_directory\\zero_dl_2\\common\\util.py:139: RuntimeWarning: invalid value encountered in log2\n",
      "  pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0% done\n",
      "2.0% done\n",
      "3.0% done\n",
      "4.0% done\n",
      "5.0% done\n",
      "6.0% done\n",
      "7.0% done\n",
      "8.0% done\n",
      "9.0% done\n",
      "10.0% done\n",
      "11.0% done\n",
      "12.0% done\n",
      "13.0% done\n",
      "14.0% done\n",
      "15.0% done\n",
      "16.0% done\n",
      "17.0% done\n",
      "18.0% done\n",
      "19.0% done\n",
      "20.0% done\n",
      "21.0% done\n",
      "22.0% done\n",
      "23.0% done\n",
      "24.0% done\n",
      "25.0% done\n",
      "26.0% done\n",
      "27.0% done\n",
      "28.0% done\n",
      "29.0% done\n",
      "30.0% done\n",
      "31.0% done\n",
      "32.0% done\n",
      "33.0% done\n",
      "34.0% done\n",
      "35.0% done\n",
      "36.0% done\n",
      "37.0% done\n",
      "38.0% done\n",
      "39.0% done\n",
      "40.0% done\n",
      "41.0% done\n",
      "42.0% done\n",
      "43.0% done\n",
      "44.0% done\n",
      "45.0% done\n",
      "46.0% done\n",
      "47.0% done\n",
      "48.0% done\n",
      "49.0% done\n",
      "50.0% done\n",
      "51.0% done\n",
      "52.0% done\n",
      "53.0% done\n",
      "54.0% done\n",
      "55.0% done\n",
      "56.0% done\n",
      "57.0% done\n",
      "58.0% done\n",
      "59.0% done\n",
      "60.0% done\n",
      "61.0% done\n",
      "62.0% done\n",
      "63.0% done\n",
      "64.0% done\n",
      "65.0% done\n",
      "66.0% done\n",
      "67.0% done\n",
      "68.0% done\n",
      "69.0% done\n",
      "70.0% done\n",
      "71.0% done\n",
      "72.0% done\n",
      "73.0% done\n",
      "74.0% done\n",
      "75.0% done\n",
      "76.0% done\n",
      "77.0% done\n",
      "78.0% done\n",
      "79.0% done\n",
      "80.0% done\n",
      "81.0% done\n",
      "82.0% done\n",
      "83.0% done\n",
      "84.0% done\n",
      "85.0% done\n",
      "86.0% done\n",
      "87.0% done\n",
      "88.0% done\n",
      "89.0% done\n",
      "90.0% done\n",
      "91.0% done\n",
      "92.0% done\n",
      "93.0% done\n",
      "94.0% done\n",
      "95.0% done\n",
      "96.0% done\n",
      "97.0% done\n",
      "98.0% done\n",
      "99.0% done\n",
      "calculating SVD ...\n",
      "\n",
      "[query] you\n",
      " i: 0.6730736494064331\n",
      " we: 0.6097985506057739\n",
      " do: 0.578296422958374\n",
      " anybody: 0.5564526915550232\n",
      " 'll: 0.527145504951477\n",
      "\n",
      "[query] year\n",
      " month: 0.6700104475021362\n",
      " last: 0.6372623443603516\n",
      " quarter: 0.632012128829956\n",
      " earlier: 0.6033249497413635\n",
      " june: 0.569846510887146\n",
      "\n",
      "[query] car\n",
      " luxury: 0.6044356822967529\n",
      " auto: 0.5109541416168213\n",
      " corsica: 0.5099695920944214\n",
      " truck: 0.506010115146637\n",
      " cars: 0.4481072425842285\n",
      "\n",
      "[query] toyota\n",
      " motor: 0.7408574819564819\n",
      " motors: 0.6647348403930664\n",
      " nissan: 0.6404471397399902\n",
      " lexus: 0.5860313177108765\n",
      " honda: 0.5836237668991089\n"
     ]
    }
   ],
   "source": [
    "# PTBデータセットに対してカウントベースの手法を適用する\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import most_similar, create_co_matrix, ppmi\n",
    "from dataset import ptb\n",
    "\n",
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print('counting co-occurrence ...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "print('calculating PPMI ...')\n",
    "W = ppmi(C, verbose=True)\n",
    "\n",
    "print('calculating SVD ...')\n",
    "try:\n",
    "    # truncated SVD(fast!)\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5, \n",
    "                            random_state=None)\n",
    "    \n",
    "except ImportError:\n",
    "    # SVD(slow)\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "    \n",
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8a658",
   "metadata": {},
   "source": [
    "## 2.5 まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddb8ec1",
   "metadata": {},
   "source": [
    "### 本章で学んだこと\n",
    "- WordNetなどのシソーラスを利用して，類義語の取得や単語間の類似度の計測など有用なタスクを行うことができる\n",
    "- シソーラスを用いる手法には，シソーラスを作成する人の作業量や新しい単語への対応などの問題がある\n",
    "- 現在ではコーパスを利用して単語をベクトル化するアプローチが主流である．\n",
    "- 近年の単語ベクトル化の手法では．「単語の意味は周囲の単語によって形成される」という分布仮説に基づくものがほとんどである．\n",
    "- カウントベースの手法は，コーパス中の各単語に対して，その単語の周囲の単語の頻度をカウントし，集計する（＝共起行列）\n",
    "- 共起行列をPPMI行列に変換し，それを次元削減することで，巨大な「疎なベクトル」を小さな「密なベクトル」へと変換することができる．\n",
    "- 単語のベクトル空間では，意味的に近い言葉はその距離が近くなることが期待される．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e764af7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
