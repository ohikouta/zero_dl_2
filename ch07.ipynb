{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17744766",
   "metadata": {},
   "source": [
    "# 7章 RNNによる文章生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd53080e",
   "metadata": {},
   "source": [
    "RNNとLSTMについて実装レベルで理解していれば，これを使ったアプリケーションの実装が可能．言語モデルの文章生成，seq2seqという新しい構造のニューラルネットワークを使用する．seq2seqは2つのRNNを組み合わせていとも簡単に実装可能．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7068870e",
   "metadata": {},
   "source": [
    "## 7.1 言語モデルを使った文章生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb670b",
   "metadata": {},
   "source": [
    "#### 言語モデルの汎用性\n",
    "機械翻訳や音声認識，文章生成などがさまざまなアプリケーションにつかうことができる．そしてそれらは作成可能．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9584c987",
   "metadata": {},
   "source": [
    "### 7.1.1 RNNによる文章生成の手順"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852b047",
   "metadata": {},
   "source": [
    "#### 言語モデルは確率分布を出力する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f19bb1",
   "metadata": {},
   "source": [
    "#### 新たに次の単語を生成するには．．\n",
    "最も高い単語を選ぶとか，確率的に選ぶとか，いろいろある．確率的に選ぶ場合は，あくまでも確率であり，決定的に一意に定まらないので，選ばれる単語は毎回異なる．\n",
    "#### 決定的とは\n",
    "- 決定的なアルゴリズム\n",
    "- 確率的なアルゴリズム\n",
    "#### そして機械は人間にとっても自然な文章を生成する\n",
    "言語モデルは訓練データを丸暗記したものではない．そこで使われる単語の並び方のパターンを学習している．言語モデルがコーパスによって単語の出現パターンを正しく学習できているのなら，言語モデルが生成する文章は．人間にとっても自然な文章になることが期待できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02f19d",
   "metadata": {},
   "source": [
    "### 7.1.2 文章生成の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4202d8",
   "metadata": {},
   "source": [
    "単語ID，確率分布を用いて実際に文章生成を行う．ランダムな重み初期値では当然意味のない文章を生成してしまうが，学習済みの重みを使用すれば，ある程度意味のわかる部分が確認できる．コードの実装はvscodeでスクリプトを作成している．gitで確認できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf13fad",
   "metadata": {},
   "source": [
    "### 7.1.3 さらに良い文章へ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da7169",
   "metadata": {},
   "source": [
    "学習後の重みをダウンロードし，正しくディレクトリに配置する．エディタでpickleファイルを見てみても相変わらずエンコード内容はわけがわからない．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec0b53e",
   "metadata": {},
   "source": [
    "## 7.2 seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245ac9f1",
   "metadata": {},
   "source": [
    "#### 入出力が共に時系列データであるケースは世の中にたくさんある．\n",
    "時系列データとして挙げられるのは，言語データ，音声データ，動画像データなどたくさんある．\n",
    "#### seq2seq\n",
    "時系列データを別の時系列データに変換するモデル．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64334005",
   "metadata": {},
   "source": [
    "### 7.2.1 seq2seqの原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b66db7",
   "metadata": {},
   "source": [
    "#### Encoder-Decoderモデル\n",
    "EncoderとDecoderが登場する，seq2seqの別の呼び名．Encoderは入力データをエンコードし，Decoderはエンコードされたデータをデコードする．読んで字のごとくといった感じ．\n",
    "#### エンコード\n",
    "情報をある規則に基づいて変換すること\n",
    "#### デコード\n",
    "エンコードされた情報を元の情報に戻すこと\n",
    "\n",
    "#### 例えばEncoderは．．\n",
    "時系列データをhという隠れ状態ベクトルに変換する．\n",
    "\n",
    "#### LSTMの隠れ状態hは固定長のベクトルである\n",
    "要するに強調したいのは，エンコードとは任意の長さの文章を固定長のベクトルに変換するということである．\n",
    "\n",
    "#### じゃあどう料理されてDecoderに？\n",
    "それをここまで見てきた．例えば，Embedding, LSTM, Affine, Softmaxといったようなアーキテクチャ．\n",
    "\n",
    "#### 唯一の小さな違い\n",
    "LSTMがベクトルhを受け取るということ．ここまで見てきたモデルではLSTMレイヤは何も受け取っていない．\n",
    "\n",
    "#### <code>eos</code>\n",
    "Decoderに文章生成の開始を知らせる合図として利用される．\n",
    "\n",
    "#### Encoder-Decoder間の架け橋\n",
    "Encoder，Decoderにそれぞれある2つのLSTMレイヤの隠れ状態こそがEncoder-Decoderの間の架け橋である．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d01772f",
   "metadata": {},
   "source": [
    "### 7.2.2 時系列データ変換用のトイ・プロブレム"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdf4245",
   "metadata": {},
   "source": [
    "#### トイ・プロブレム\n",
    "機械学習を評価するために作られた簡単な問題．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d71e60c",
   "metadata": {},
   "source": [
    "### 7.2.3 可変長の時系列データ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac780e3",
   "metadata": {},
   "source": [
    "#### 問題ごとに文字数が異なるなら．．\n",
    "サンプルごとにデータの時間方向のサイズが異なるなら，それは可変長の時系列データを扱うことであり，ニューラルネットワークの学習におけるミニバッチ処理を行うために何らかの工夫が必要．\n",
    "\n",
    "#### パディング\n",
    "本来のデータを無効なデータで埋め，データの長さを均一に揃えるテクニック\n",
    "\n",
    "#### 区切り文字\n",
    "Decoderに文字列生成を知らせる合図として使われる．\n",
    "\n",
    "#### パディングを用いるなら．．\n",
    "本来存在しなかったパディング用の文字までseq2seqに処理させることになる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2594d6",
   "metadata": {},
   "source": [
    "### 7.2.4 足し算データセット"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae13e1f",
   "metadata": {},
   "source": [
    "## 7.3 seq2seqの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ff7c80",
   "metadata": {},
   "source": [
    "#### seq2seqのざっくりしたアーキテクチャ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3cedfd",
   "metadata": {},
   "source": [
    "### 7.3.1 Encoderクラス"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4abb583",
   "metadata": {},
   "source": [
    "#### Encoderクラスのアーキテクチャ\n",
    "RNNを用いてEncoderを実現する．EmbeddingレイヤとLSTMレイヤによって構成される．\n",
    "\n",
    "#### LSTMレイヤは，右方向（時間方向）には隠れ状態とセルを出力．上方向には隠れ状態のみ出力．上方向にレイヤはないので破棄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e22106",
   "metadata": {},
   "source": [
    "#### 初期化メソッド\n",
    "重みパラメータの初期化とレイヤの生成を行う．長い時系列データが一つだけ存在する問題として扱った際は<code>stateful=True</code>を指定して隠れ状態を維持したまま長い時系列データを処理した．今回は短い時系列データが複数存在する問題なので，問題ごとにLSTMの隠れ状態をリセットする必要がある．\n",
    "\n",
    "#### メソッドの機能と理論\n",
    "メソッドの詳細な挙動は，もうすでに学習済みとしてここでは説明を焼灼している．しかし．その機能の内容はただテキストを読んで一周しただけでは当然つかめない．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb2517b",
   "metadata": {},
   "source": [
    "### 7.3.2 Decoderクラス"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee092a2b",
   "metadata": {},
   "source": [
    "#### DecoderもEncoderと同様にRNNを使って実装する．\n",
    "\n",
    "#### 学習時と生成時ではデータの与え方が異なる\n",
    "学習時は正解がわかっているため，時系列方向のデータをまとめて与えることができる．一方推論時には，新しい文字列を生成するときには最初に開始を知らせる区切り文字を一つ与える．そしてその時の出力から文字を一つサンプリングし，それを次の入力にするという処理を繰り返し行う．\n",
    "\n",
    "#### 決定的に文字列を生成させる．\n",
    "最も高いスコアを持つ文字を一つ選ぶ．これまでに説明した確率的な考え方と対になる決定的な考え方のこと．\n",
    "\n",
    "#### argmaxの追加\n",
    "最大値を取るインデックス\n",
    "\n",
    "#### softmax省略\n",
    "softmaxは入力されたベクトルを正規化し，ベクトルの各要素の値は変換されるが．大小関係は変動がないためsoftmaxを省略し，Affineの出力をargmaxを適用する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e0338d",
   "metadata": {},
   "source": [
    "### 7.3.3 seq2seqクラス"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b33cfde",
   "metadata": {},
   "source": [
    "#### EncoderクラスとDecoderクラスをつなぎ合わせ，TimeSoftmaxwithLossレイヤを使って損失を計算するだけ．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec7912",
   "metadata": {},
   "source": [
    "### 7.3.4 seq2seqの評価"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9895cdb",
   "metadata": {},
   "source": [
    "#### 基本的なニューラルネットワークの学習を同じように学習が進められる\n",
    "- 1. 学習データからミニバッチを選び\n",
    "- 2. ミニバッチから勾配を計算し，\n",
    "- 3. 勾配を使ってパラメータを更新する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7582735",
   "metadata": {},
   "source": [
    "## 7.4 seq2seqの改良"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2433c",
   "metadata": {},
   "source": [
    "学習の進みを改善するための有望なテクニックはいくつか存在する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c08d0",
   "metadata": {},
   "source": [
    "### 7.4.1 入力データの反転（Reverse）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b1c744",
   "metadata": {},
   "source": [
    "入力データを反転させるだけで，学習進度に大きく影響がある．理論的な根拠は説明できない．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333d212",
   "metadata": {},
   "source": [
    "### 7.4.2 覗き見（Peeky）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc706e7d",
   "metadata": {},
   "source": [
    "重要な情報を占有せずに，みんなで共有しようというもの．Affine，LSTMレイヤにEncoderの出力hを与える．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d852a",
   "metadata": {},
   "source": [
    "## 7.5 seq2seqを用いたアプリケーション"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f5d820",
   "metadata": {},
   "source": [
    "#### ある時系列データを別の時系列データに変換する\n",
    "- 機械翻訳：ある言語の文章を別の言語の文章に変換する\n",
    "- 自動要約：ある長い文章を短い要約された文章に変換する\n",
    "- 質疑応答：質問を答えに変換する\n",
    "- メールの自動返信：受け取ったメールの文章を返信文に変換する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8100ad01",
   "metadata": {},
   "source": [
    "### 7.5.1 チャットボット"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16291dd8",
   "metadata": {},
   "source": [
    "### 7.5.2 アルゴリズムの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a429c",
   "metadata": {},
   "source": [
    "### 7.5.3 イメージキャプション"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30898e97",
   "metadata": {},
   "source": [
    "## 7.6 まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9629cab",
   "metadata": {},
   "source": [
    "#### 本章で学んだこと\n",
    "- RNNを用いた言語モデルは新しい文章を生成することができる\n",
    "- 文章生成を行う際には，ひとつの単語（もしくは文字）を与え，モデルの出力（確率分布）からサンプリングするという手順を繰り返し行う\n",
    "- RNNを2つ組み合わせることで，時系列データを別の時系列データに変換することができる（seq2seq）\n",
    "- seq2seqは，Encoderが入力文をエンコードし，そのエンコード情報をDecoderが受け取り，デコードして目的の出力文を得る\n",
    "- 入力文を反転させること（Reverse），またエンコード情報をDecoderの複数のレイヤに与えること（Peeky）は，seq2seqの精度向上に有効である\n",
    "- 機械翻訳やチャットボット，イメージキャプションなど，seq2seqはさまざまなアプリケーションに利用できる"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
