{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cdbc399",
   "metadata": {},
   "source": [
    "# 1章 ニューラルネットワークの復習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025b853",
   "metadata": {},
   "source": [
    "## 1.1 数学とPythonの復習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99165f28",
   "metadata": {},
   "source": [
    "### 1.1.1 ベクトルと行列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566f7c96",
   "metadata": {},
   "source": [
    "テンソル：ベクトルや行列を拡張させ，N次元数の集まりとみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af278477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(3,)\n",
      "1\n",
      "(2, 3)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ベクトルの定義\n",
    "x = np.array([1, 2, 3])\n",
    "print(x.__class__)  # クラスを表示\n",
    "\n",
    "print(x.shape)   # 形状の確認\n",
    "print(x.ndim)    # 次元数の確認\n",
    "\n",
    "# 行列の定義\n",
    "W = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(W.shape)  # 形状の確認\n",
    "print(W.ndim)   # 次元数の確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bfcabc",
   "metadata": {},
   "source": [
    "### 1.1.2 行列の要素ごとの計算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd47f11a",
   "metadata": {},
   "source": [
    "要素ごとの計算をしてみる．\n",
    "このとき，多次元配列中の要素の中で，-各要素が独立して- 演算が行われる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6d04b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  5]\n",
      " [ 7  9 11]]\n",
      "[[ 0  2  6]\n",
      " [12 20 30]]\n",
      "[[       inf 2.         1.5       ]\n",
      " [1.33333333 1.25       1.2       ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\81804\\AppData\\Local\\Temp/ipykernel_21104/1312443076.py:6: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  print(W / X)  # divide by zero\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "X = np.array([[0, 1, 2], [3, 4, 5]])\n",
    "\n",
    "print(W + X)\n",
    "print(W * X)\n",
    "print(W / X)  # divide by zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d5927",
   "metadata": {},
   "source": [
    "### 1.1.3 ブロードキャスト"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f843f",
   "metadata": {},
   "source": [
    "ブロードキャスト：形状の異なる配列同士の演算を可能にする．NumPyの賢い機能．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfc4a5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 20]\n",
      " [30 40]]\n",
      "[[10 40]\n",
      " [30 80]]\n"
     ]
    }
   ],
   "source": [
    "# 2次元配列とスカラーの計算\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "print(A * 10)\n",
    "\n",
    "# 1次元配列と2次元配列を計算する\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([10, 20])\n",
    "print(A * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8a51e4",
   "metadata": {},
   "source": [
    "##### NumPyのブロードキャストが有効に働くには，多次元配列のいくつかのルールを満たす必要がある．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a67250d",
   "metadata": {},
   "source": [
    "### 1.1.4 ベクトルの内積と行列の積"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caf6276",
   "metadata": {},
   "source": [
    "- ベクトルの内積：2つのベクトル間の対応する要素の積を足し合わせたもの．直観的には「2つのベクトルがどれだけ同じ方向を向いているか」を表すと考えられる．\n",
    "- 行列の積：左側の行列の行ベクトル（横方向）と右側の行列の列ベクトル（縦方向）の内積（要素ごとの積と和）によって計算される．計算結果は新しい行列の対応する要素に格納される．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c138303f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "[[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "# ベクトルの積\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "print(np.dot(a, b))\n",
    "\n",
    "# 行列の積\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "print(np.dot(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b30ea9",
   "metadata": {},
   "source": [
    "形状チェック：行列の積では，対応する次元の要素数を一致させる．\n",
    "\n",
    "行列の積などの計算では，形状チェック- 行列の形状に注目し，その推移を見ていくことが- 重要．それによってニューラルネットワークの実装をスムーズに進めることができる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5343d",
   "metadata": {},
   "source": [
    "## 1.2 ニューラルネットワークの推論"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343161a2",
   "metadata": {},
   "source": [
    "### 1.2.1 ニューラルネットワーク推論の全体図"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb03fe65",
   "metadata": {},
   "source": [
    "全結合層：隣接するニューロン間のすべてに（矢印による）結びつきがあるニューラルネットワーク\n",
    "\n",
    "隠れ層のニューロンは重みをニューロンの乗算にバイアスを加算した重み付き和で求めることができる．\n",
    "\n",
    "重み付き和は行列の積でまとめて計算することができる．\n",
    "\n",
    "ニューラルネットワークの分野では，複数のサンプルデータ（ミニバッチ）に対して，一斉に推論や学習を行う．それを行うために行列xの各行に，個別のサンプルデータを格納する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e597c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41740709  1.12897949]\n",
      " [-0.01073021  0.71204722]\n",
      " [-0.96060348 -0.93625724]\n",
      " [ 0.16412077 -2.04854319]\n",
      " [-0.85810279  0.81632931]\n",
      " [-1.015187   -0.40560089]\n",
      " [-0.58159555  0.51073976]\n",
      " [-1.36675517  0.0946858 ]\n",
      " [ 1.92960725 -0.38318468]\n",
      " [ 1.74132828  0.05298426]]\n",
      "[[-1.20691962 -0.10938825  0.7410933  -1.55563603]\n",
      " [-1.32946119 -0.80444209 -0.06819892 -0.78671374]\n",
      " [-2.44705248 -2.32451884  1.09915342  0.0299136 ]\n",
      " [-2.76411539 -4.19787774 -1.1278136   2.11582765]\n",
      " [-1.48339604 -0.27265537  1.40732606 -1.62720013]\n",
      " [-2.17646683 -1.66214575  1.33864659 -0.54625999]\n",
      " [-1.57855111 -0.7717412   0.85097764 -1.08236637]\n",
      " [-1.99565779 -0.89361762  2.07682845 -1.35033   ]\n",
      " [-1.43549142 -3.04887967 -3.6830143   1.99118478]\n",
      " [-1.2485874  -2.43560305 -3.24130376  1.39293943]]\n"
     ]
    }
   ],
   "source": [
    "# ミニバッチ版の全結合層による変換\n",
    "import numpy as np\n",
    "W1 = np.random.randn(2, 4)  # 重み\n",
    "b1 = np.random.randn(4)     # バイアス\n",
    "x = np.random.randn(10, 2)  # 入力\n",
    "h = np.dot(x, W1) + b1\n",
    "print(x)\n",
    "print(h)  # shape(10, 4)のデータが格納されている"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a802f",
   "metadata": {},
   "source": [
    "上のコードにおいてはバイアスの計算においてブロードキャストが働いている．\n",
    "\n",
    "全結合層による変換は「線形」な変換．これに「非線形」な効果を与えるのが，活性化関数．非線形な活性化関数を用いることで，ニューラルネットワークの表現力は増すことができる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1ec6c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23024654 0.47268017 0.67723488 0.17427374]\n",
      " [0.2092485  0.30907611 0.48295688 0.31287473]\n",
      " [0.07965436 0.08911258 0.75010145 0.50747784]\n",
      " [0.0592944  0.01480495 0.24456482 0.89243205]\n",
      " [0.18491502 0.43225532 0.80334385 0.16421428]\n",
      " [0.10188377 0.15947417 0.79226729 0.36673255]\n",
      " [0.17100078 0.31610257 0.70077218 0.25305846]\n",
      " [0.11965958 0.29036384 0.88863054 0.20581643]\n",
      " [0.1922445  0.04526587 0.0245302  0.87986843]\n",
      " [0.22294476 0.08049777 0.03764063 0.80106109]]\n"
     ]
    }
   ],
   "source": [
    "# sigmoid関数を実装\n",
    "# sigmoid関数は，任意の実数を受け取り，0～1の間の実数を出力する\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 隠れ層のニューロンをsigmoid関数で変換する\n",
    "\n",
    "a = sigmoid(h)\n",
    "print(a)  # アクティベーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c89c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここまでの話をまとめた処理を実装\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.random.randn(10, 2)\n",
    "W1 = np.random.randn(2, 4)\n",
    "b1 = np.random.randn(4)\n",
    "W2 = np.random.randn(4, 3)\n",
    "b2 = np.random.randn(3)\n",
    "\n",
    "h = np.dot(x, W1) + b1\n",
    "a = sigmoid(h)\n",
    "s = np.dot(a, W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87f33d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.84742578  1.0122926  -1.5386459 ]\n",
      " [ 1.84033582  0.30078804 -1.06474079]\n",
      " [ 1.73700448 -0.1368664  -0.85099286]\n",
      " [ 1.03904554 -0.71522833 -0.49051237]\n",
      " [ 2.92219957  0.46265745 -1.28582365]\n",
      " [ 2.73269102  1.03081556 -1.53346234]\n",
      " [ 2.66298935  0.767315   -1.39314517]\n",
      " [ 2.22147193  0.43765442 -1.17849409]\n",
      " [ 2.93030975  0.74054343 -1.41868277]\n",
      " [ 3.15075296  1.24109293 -1.7027028 ]]\n"
     ]
    }
   ],
   "source": [
    "print(s)\n",
    "\n",
    "# softmax関数を用いると，スコアの値が確率に変換される．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6171fd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.8474257804354237, 1.012292601590684, -1.538645896026586]\n",
      "[1.840335819262029, 0.3007880369659264, -1.0647407880742514]\n",
      "[1.7370044792996469, -0.13686639832591405, -0.8509928638847406]\n",
      "[1.039045535620169, -0.7152283345043766, -0.49051237124569125]\n",
      "[2.9221995696267253, 0.46265744851252066, -1.2858236501016425]\n",
      "[2.732691019136998, 1.030815564616516, -1.5334623416887703]\n",
      "[2.6629893546179946, 0.7673149974895943, -1.3931451676330011]\n",
      "[2.221471931402277, 0.43765442207503175, -1.1784940874711873]\n",
      "[2.9303097516381595, 0.7405434321808437, -1.4186827741560446]\n",
      "[3.1507529607939735, 1.2410929305382943, -1.702702797584103]\n",
      "['yellow', 'yellow', 'yellow', 'blue', 'yellow', 'yellow', 'yellow', 'yellow', 'yellow', 'yellow']\n"
     ]
    }
   ],
   "source": [
    "# 出力の最大値を推論結果とする分類問題を作成\n",
    "\n",
    "class_list = [\"red\", \"blue\", \"yellow\"]  # 3つのクラスに分類\n",
    "# ndarrayをlistに変換\n",
    "t = s.tolist()\n",
    "# 結果を格納するリスト\n",
    "result = []\n",
    "\n",
    "# for文で各配列の最小値を取り出し，該当するクラスを出力としてresultに追加\n",
    "for i in t:\n",
    "    print(i)\n",
    "    result_num = min(i)\n",
    "    result.append(class_list[i.index(result_num)])\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5f09c",
   "metadata": {},
   "source": [
    "### 1.2.2 レイヤとしてのクラス化と順伝播の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d65ed",
   "metadata": {},
   "source": [
    "ニューラルネットワークで行う処理を「レイヤ」として実装する．\n",
    "- Affineレイヤ：全結合層による変換\n",
    "- Sigmoidレイヤ：シグモイド関数による変換\n",
    "\n",
    "各レイヤはPythonのクラスとして実装する．メインとなる変換をforward()というメソッド名で実装する．\n",
    "\n",
    "ニューラルネットワークの推論で行う処理は，ニューラルネットワークの順伝播に相当する．ニューラルネットワークでの学習の差異には，順伝播とは逆方向にデータ（勾配）を伝播する．これは逆伝播という．\n",
    "\n",
    "##### ニューラルネットワークの実装ルールを設ける\n",
    "- すべてのレイヤは，メソッドとしてforward()とbackward()を持つ\n",
    "- すべてのレイヤは，インスタンス変数としてparamsとgradsを持つ\n",
    "\n",
    "forward(),backward()：それぞれが順伝播と逆伝播に対応する.\n",
    "\n",
    "paramsは重みやバイアスなどのパラメータをリストとして保持する．\n",
    "\n",
    "gradsはparamsのパラメータに対応する形で，各パラメータの勾配をリストとして保持する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b9b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoidレイヤを実装\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        # 学習するパラメータは存在しないのでparamsを空のリストで初期化\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "# Affineレイヤを実装\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3777253",
   "metadata": {},
   "source": [
    "Affineレイヤは，初期化時に重みとバイアスを受け取る．このとき，Affineレイヤのパラメータは，重みとバイアスになる（その2つのパラメータが，ニューラルネットワークの学習で随時更新）．この2つはインスタンス変数のparamsにリストとして保存される．\n",
    "\n",
    "実装ルールに従うと，すべてのレイヤには学習すべきパラメータがインスタンス変数のparamsに必ず存在することになる．そのため，ニューラルネットワークのすべてのパラメータを簡単にまとめることができ，パラメータの更新作業やパラメータのファイルへの保存が容易になる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "137628fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラルネットワークの推論処理を実装する\n",
    "# 流れはAffine→Sigmoid→Affine\n",
    "# TwoLayerNetというクラスでメインの推論処理をpredict(x)というメソッドで実装\n",
    "\n",
    "# TwoLayerNet\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        \n",
    "        # 重みとバイアスの初期化\n",
    "        W1 = np.random.randn(I, H)\n",
    "        b1 = np.random.randn(H)\n",
    "        W2 = np.random.randn(H, O)\n",
    "        b2 = np.random.randn(O)\n",
    "        \n",
    "        # レイヤの作成\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        \n",
    "        # すべての重みをリストにまとめる\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6de49bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64515508 -1.42253412  0.16079912]\n",
      " [-0.10862677 -2.73336097  1.69908848]\n",
      " [-0.04295792 -2.20195024  0.65467227]\n",
      " [-0.09217282 -2.00574843 -0.17149744]\n",
      " [ 0.56596568 -1.73494578  1.05524889]\n",
      " [ 0.06319005 -2.06098324  0.45045015]\n",
      " [ 0.27182784 -1.99515819  1.88272134]\n",
      " [ 0.04703186 -2.04102781  0.26824644]\n",
      " [ 0.23059266 -2.08385397  2.55353561]\n",
      " [ 0.05973775 -2.07720559  0.54786167]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)\n",
    "print(s)  # 結果の出力\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f323c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.30125433, -0.1397623 ,  2.06150556, -0.60246427],\n",
      "       [ 2.32813302,  0.98482191,  0.19927845,  0.13604085]]), array([-1.37381613,  1.8201828 , -0.97010266, -1.12441416]), array([[-0.46888568, -0.30072422,  2.02923588],\n",
      "       [ 0.28741483, -0.25905885,  1.20321348],\n",
      "       [ 1.36016433,  0.62307928,  0.74492336],\n",
      "       [ 1.03460506, -0.95211515,  0.12803831]]), array([-0.66728962, -1.69731335, -0.99425519])]\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習すべきパラメータがリストにまとめられている．\n",
    "ls = model.params\n",
    "print(ls)\n",
    "print(type(ls))\n",
    "len(ls)  # 重みバイアス重みバイアスで4種類"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c62cd",
   "metadata": {},
   "source": [
    "## 1.3 ニューラルネットワークの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0cc2b9",
   "metadata": {},
   "source": [
    "ニューラルネットワークは学習を行わなければよい推論ができない．最初に学習を行い，その学習されたパラメータを使って推論するのが一般的な流れである．推論は，他クラス分類などの問題に答えを出す作業であり，学習は，最適なパラメータを見つける作業である．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157dc264",
   "metadata": {},
   "source": [
    "### 1.3.1 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3425c7",
   "metadata": {},
   "source": [
    "多クラス分類を行うニューラルネットワークの場合，損失関数として交差エントロピー誤差を用いる場合が多くある．交差エントロピー誤差は，ニューラルネットワークが出力する各クラスの「確率」と「教師ラベル」から求められる．\n",
    "\n",
    "##### 損失を求めるために，SoftmaxレイヤとCrossEntoropyErrorレイヤを追加する．\n",
    "\n",
    "Softmax関数と交差エントロピー誤差を計算するレイヤをSoftmaxwithLossレイヤとして実装する．（まとめることで逆伝播の計算が簡単になるため）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019212e",
   "metadata": {},
   "source": [
    "### 1.3.2 微分と勾配"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddf77cf",
   "metadata": {},
   "source": [
    "### 1.3.3 チェインルール"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05cb825",
   "metadata": {},
   "source": [
    "#### チェインルールの重要ポイント\n",
    "私たちの扱う関数がどれだけ複雑だとしても，どれだけ複数の関数が連結したとしても，その微分は個別の関数の微分によって求めることができるということ．各関数の局所的な微分を計算できれば，その積によって最終的な全体の微分を求めることができる．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f925094",
   "metadata": {},
   "source": [
    "### 1.3.4 計算グラフ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06352267",
   "metadata": {},
   "source": [
    "計算グラフ：計算を視覚的に表すもの"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ace1a",
   "metadata": {},
   "source": [
    "- 乗算ノード\n",
    "- 分岐ノード\n",
    "- Repeatノード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a91e0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeatノード\n",
    "import numpy as np\n",
    "D, N = 8, 7\n",
    "x = np.random.randn(1, D)  # 入力\n",
    "y = np.repeat(x, N, axis=0)  # forward\n",
    "\n",
    "dy = np.random.randn(N, D)  # 仮の勾配\n",
    "dx = np.sum(dy, axis=0, keepdims=True)\n",
    "\n",
    "dx.shape  # keepdims=Trueの場合は(1, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a15b3b7",
   "metadata": {},
   "source": [
    "- Sumノード(汎用的な加算ノード)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c4b803a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 8)\n",
      "(1, 8)\n"
     ]
    }
   ],
   "source": [
    "# Sumノード\n",
    "import numpy as np\n",
    "D, N = 8, 7\n",
    "x = np.random.randn(N, D)\n",
    "print(x.shape)\n",
    "y = np.sum(x, axis=0, keepdims=True)  # forward\n",
    "\n",
    "dy = np.random.randn(1, D)  # 仮の勾配\n",
    "print(dy.shape)\n",
    "dx = np.repeat(dy, N, axis=0)  # backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ceb68",
   "metadata": {},
   "source": [
    "SumノードとRepeatノードは，それぞれ逆の関係にある．Sumノードの順伝播がRepeatノードの逆伝播に相当し，Sumノードの逆伝播がRepeatノードの順伝播に相当する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deef85a4",
   "metadata": {},
   "source": [
    "- MatMulノード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc038066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MatMulノードをレイヤとして実装する\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dw = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6fff34",
   "metadata": {},
   "source": [
    "### 1.3.5 勾配の導出と逆伝播の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e225c",
   "metadata": {},
   "source": [
    "- Sigmoidレイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5dbc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoidレイヤ\n",
    "# 順伝播の出力をインスタンス変数のoutに保持しておく．\n",
    "# 逆伝播の際にout変数を使って計算を行う．\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Sigmoidを計算するだけ\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ba2093",
   "metadata": {},
   "source": [
    "- Affineレイヤ\n",
    "\n",
    "バイアスの加算では，NumPyのブロードキャストが使われている．その点を明示的に記述して実装する必要がある．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a02e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPyのブロードキャスト機能は，裏側でRepeatノードの計算が行われていると考える\n",
    "# Affineレイヤの実装\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np_zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout. W.T)\n",
    "        dw = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf98eca",
   "metadata": {},
   "source": [
    "- Softmax with Lossレイヤ\n",
    "\n",
    "#### ニューラルネットワークの学習における重要な性質\n",
    "Softmaxレイヤから逆伝播は，Softmaxレイヤの出力と教師ラベルの差分になる．ニューラルネットワークの逆伝播ではこの差分（誤差）が前レイヤへ伝わっていく．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e61dc",
   "metadata": {},
   "source": [
    "### 1.3.6 重みの更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b18d7a",
   "metadata": {},
   "source": [
    "誤差逆伝播法によって勾配を求めることができたら，その勾配を使ってニューラルネットワークのパラメータを更新する．\n",
    "- step1(ミニバッチ):訓練データの中からランダムに複数のデータを選び出す\n",
    "- step2(勾配の算出):誤差逆伝播法により，各重みパラメータに関する損失関数の勾配を求める．\n",
    "- step3(パラメータの更新):勾配を使って重みパラメータを更新する\n",
    "- step4(繰り返す):step-1, step-2, step-3を必要な回数だけ繰り返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d6a47",
   "metadata": {},
   "source": [
    "#### 勾配降下法\n",
    "まずはミニバッチでデータを選び，続いて誤差逆伝播法によって重みの勾配を得る．この勾配は，現時点での重みパラメータにおいて，損失を最も増やす方向を指す．そのため，パラメータをその購買の逆方向に更新することで，損失を下げる．この作業を必要な回数だけ繰り返す．\n",
    "#### SGD(Stochastic Gradient Descent: 確率的勾配降下法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4a267ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGDの実装\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948229ab",
   "metadata": {},
   "source": [
    "初期化の引数であるlrはlearning rate（学習係数）を表す．学習係数をインスタンス変数として保持する．update(params, grads)というメソッドでパラメータの更新処理を行う．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60323974",
   "metadata": {},
   "source": [
    "## 1.4 ニューラルネットワークで問題を解く"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1baf8c2",
   "metadata": {},
   "source": [
    "### 1.4.1 スパイラル・データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec123fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # 親ディレクトリのファイルをインポートするための設定\n",
    "from dataset import spiral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ccc9dbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Duplicate key in file WindowsPath('C:/Users/81804/anaconda3/lib/site-packages/matplotlib/mpl-data/matplotlibrc'), line 258 ('font.family:  sans-serif')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (300, 2)\n",
      "t (300, 3)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# xが入力データ，tが教師ラベル\n",
    "# データの読み込みを行う\n",
    "\n",
    "x, t = spiral.load_data()\n",
    "print('x', x.shape)\n",
    "print('t', t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a04142b",
   "metadata": {},
   "source": [
    "### 1.4.2 ニューラルネットワークの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25c64f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラルネットワークを実装\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import Affine, Sigmoid, SoftmaxWithLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5141313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "        \n",
    "        # 重みとバイアスの初期化\n",
    "        W1 = 0.01 * np.random.randn(I, H)\n",
    "        b1 = np.zeros(H)\n",
    "        W2 = 0.01 * np.random.randn(H, O)\n",
    "        b2 = np.zeros(O)\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "    # 3つのメソッドを実装\n",
    "    \n",
    "    # 推論を行うpredict()\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    # 順伝播を行うforward()\n",
    "    def forward(self, x, t):\n",
    "        score = self.predict(x)\n",
    "        loss = self.loss_layer.forward(score, t)\n",
    "        return loss\n",
    "\n",
    "    # 逆伝播を行うbackward()\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8ab0d3",
   "metadata": {},
   "source": [
    "ここでの実装がすっきりとしているのは，これまでにニューラルネットワークで使用する処理ブロックをレイヤとして実装してきたことが功を奏している．forward, backwardを適切な順番で呼び出すだけでいい．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff98d2",
   "metadata": {},
   "source": [
    "### 1.4.3 学習用のソースコード"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095c78b2",
   "metadata": {},
   "source": [
    "学習データを読み込み，ニューラルネットワーク（モデル）とオプティマイザを生成する．先に示した学習の4ステップの手順に従って学習を行う．機械学習の分野では，問題のために設計した手法（ニューラルネットワークやSVMなど）を指して「モデル」と呼ぶことが一般的である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b8f86459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 | iter 10 / 10 | loss 1.13\n",
      "| epoch 2 | iter 10 / 10 | loss 1.13\n",
      "| epoch 3 | iter 10 / 10 | loss 1.12\n",
      "| epoch 4 | iter 10 / 10 | loss 1.12\n",
      "| epoch 5 | iter 10 / 10 | loss 1.11\n",
      "| epoch 6 | iter 10 / 10 | loss 1.14\n",
      "| epoch 7 | iter 10 / 10 | loss 1.16\n",
      "| epoch 8 | iter 10 / 10 | loss 1.11\n",
      "| epoch 9 | iter 10 / 10 | loss 1.12\n",
      "| epoch 10 | iter 10 / 10 | loss 1.13\n",
      "| epoch 11 | iter 10 / 10 | loss 1.12\n",
      "| epoch 12 | iter 10 / 10 | loss 1.11\n",
      "| epoch 13 | iter 10 / 10 | loss 1.09\n",
      "| epoch 14 | iter 10 / 10 | loss 1.08\n",
      "| epoch 15 | iter 10 / 10 | loss 1.04\n",
      "| epoch 16 | iter 10 / 10 | loss 1.03\n",
      "| epoch 17 | iter 10 / 10 | loss 0.96\n",
      "| epoch 18 | iter 10 / 10 | loss 0.92\n",
      "| epoch 19 | iter 10 / 10 | loss 0.92\n",
      "| epoch 20 | iter 10 / 10 | loss 0.87\n",
      "| epoch 21 | iter 10 / 10 | loss 0.85\n",
      "| epoch 22 | iter 10 / 10 | loss 0.82\n",
      "| epoch 23 | iter 10 / 10 | loss 0.79\n",
      "| epoch 24 | iter 10 / 10 | loss 0.78\n",
      "| epoch 25 | iter 10 / 10 | loss 0.82\n",
      "| epoch 26 | iter 10 / 10 | loss 0.78\n",
      "| epoch 27 | iter 10 / 10 | loss 0.76\n",
      "| epoch 28 | iter 10 / 10 | loss 0.76\n",
      "| epoch 29 | iter 10 / 10 | loss 0.78\n",
      "| epoch 30 | iter 10 / 10 | loss 0.75\n",
      "| epoch 31 | iter 10 / 10 | loss 0.78\n",
      "| epoch 32 | iter 10 / 10 | loss 0.77\n",
      "| epoch 33 | iter 10 / 10 | loss 0.77\n",
      "| epoch 34 | iter 10 / 10 | loss 0.78\n",
      "| epoch 35 | iter 10 / 10 | loss 0.75\n",
      "| epoch 36 | iter 10 / 10 | loss 0.74\n",
      "| epoch 37 | iter 10 / 10 | loss 0.76\n",
      "| epoch 38 | iter 10 / 10 | loss 0.76\n",
      "| epoch 39 | iter 10 / 10 | loss 0.73\n",
      "| epoch 40 | iter 10 / 10 | loss 0.75\n",
      "| epoch 41 | iter 10 / 10 | loss 0.76\n",
      "| epoch 42 | iter 10 / 10 | loss 0.76\n",
      "| epoch 43 | iter 10 / 10 | loss 0.76\n",
      "| epoch 44 | iter 10 / 10 | loss 0.74\n",
      "| epoch 45 | iter 10 / 10 | loss 0.75\n",
      "| epoch 46 | iter 10 / 10 | loss 0.73\n",
      "| epoch 47 | iter 10 / 10 | loss 0.72\n",
      "| epoch 48 | iter 10 / 10 | loss 0.73\n",
      "| epoch 49 | iter 10 / 10 | loss 0.72\n",
      "| epoch 50 | iter 10 / 10 | loss 0.72\n",
      "| epoch 51 | iter 10 / 10 | loss 0.72\n",
      "| epoch 52 | iter 10 / 10 | loss 0.72\n",
      "| epoch 53 | iter 10 / 10 | loss 0.74\n",
      "| epoch 54 | iter 10 / 10 | loss 0.74\n",
      "| epoch 55 | iter 10 / 10 | loss 0.72\n",
      "| epoch 56 | iter 10 / 10 | loss 0.72\n",
      "| epoch 57 | iter 10 / 10 | loss 0.71\n",
      "| epoch 58 | iter 10 / 10 | loss 0.70\n",
      "| epoch 59 | iter 10 / 10 | loss 0.72\n",
      "| epoch 60 | iter 10 / 10 | loss 0.70\n",
      "| epoch 61 | iter 10 / 10 | loss 0.71\n",
      "| epoch 62 | iter 10 / 10 | loss 0.72\n",
      "| epoch 63 | iter 10 / 10 | loss 0.70\n",
      "| epoch 64 | iter 10 / 10 | loss 0.71\n",
      "| epoch 65 | iter 10 / 10 | loss 0.73\n",
      "| epoch 66 | iter 10 / 10 | loss 0.70\n",
      "| epoch 67 | iter 10 / 10 | loss 0.71\n",
      "| epoch 68 | iter 10 / 10 | loss 0.69\n",
      "| epoch 69 | iter 10 / 10 | loss 0.70\n",
      "| epoch 70 | iter 10 / 10 | loss 0.71\n",
      "| epoch 71 | iter 10 / 10 | loss 0.68\n",
      "| epoch 72 | iter 10 / 10 | loss 0.69\n",
      "| epoch 73 | iter 10 / 10 | loss 0.67\n",
      "| epoch 74 | iter 10 / 10 | loss 0.68\n",
      "| epoch 75 | iter 10 / 10 | loss 0.67\n",
      "| epoch 76 | iter 10 / 10 | loss 0.66\n",
      "| epoch 77 | iter 10 / 10 | loss 0.69\n",
      "| epoch 78 | iter 10 / 10 | loss 0.64\n",
      "| epoch 79 | iter 10 / 10 | loss 0.68\n",
      "| epoch 80 | iter 10 / 10 | loss 0.64\n",
      "| epoch 81 | iter 10 / 10 | loss 0.64\n",
      "| epoch 82 | iter 10 / 10 | loss 0.66\n",
      "| epoch 83 | iter 10 / 10 | loss 0.62\n",
      "| epoch 84 | iter 10 / 10 | loss 0.62\n",
      "| epoch 85 | iter 10 / 10 | loss 0.61\n",
      "| epoch 86 | iter 10 / 10 | loss 0.60\n",
      "| epoch 87 | iter 10 / 10 | loss 0.60\n",
      "| epoch 88 | iter 10 / 10 | loss 0.61\n",
      "| epoch 89 | iter 10 / 10 | loss 0.59\n",
      "| epoch 90 | iter 10 / 10 | loss 0.58\n",
      "| epoch 91 | iter 10 / 10 | loss 0.56\n",
      "| epoch 92 | iter 10 / 10 | loss 0.56\n",
      "| epoch 93 | iter 10 / 10 | loss 0.54\n",
      "| epoch 94 | iter 10 / 10 | loss 0.53\n",
      "| epoch 95 | iter 10 / 10 | loss 0.53\n",
      "| epoch 96 | iter 10 / 10 | loss 0.52\n",
      "| epoch 97 | iter 10 / 10 | loss 0.51\n",
      "| epoch 98 | iter 10 / 10 | loss 0.50\n",
      "| epoch 99 | iter 10 / 10 | loss 0.48\n",
      "| epoch 100 | iter 10 / 10 | loss 0.48\n",
      "| epoch 101 | iter 10 / 10 | loss 0.46\n",
      "| epoch 102 | iter 10 / 10 | loss 0.45\n",
      "| epoch 103 | iter 10 / 10 | loss 0.45\n",
      "| epoch 104 | iter 10 / 10 | loss 0.44\n",
      "| epoch 105 | iter 10 / 10 | loss 0.44\n",
      "| epoch 106 | iter 10 / 10 | loss 0.41\n",
      "| epoch 107 | iter 10 / 10 | loss 0.40\n",
      "| epoch 108 | iter 10 / 10 | loss 0.41\n",
      "| epoch 109 | iter 10 / 10 | loss 0.40\n",
      "| epoch 110 | iter 10 / 10 | loss 0.40\n",
      "| epoch 111 | iter 10 / 10 | loss 0.38\n",
      "| epoch 112 | iter 10 / 10 | loss 0.38\n",
      "| epoch 113 | iter 10 / 10 | loss 0.36\n",
      "| epoch 114 | iter 10 / 10 | loss 0.37\n",
      "| epoch 115 | iter 10 / 10 | loss 0.35\n",
      "| epoch 116 | iter 10 / 10 | loss 0.34\n",
      "| epoch 117 | iter 10 / 10 | loss 0.34\n",
      "| epoch 118 | iter 10 / 10 | loss 0.34\n",
      "| epoch 119 | iter 10 / 10 | loss 0.33\n",
      "| epoch 120 | iter 10 / 10 | loss 0.34\n",
      "| epoch 121 | iter 10 / 10 | loss 0.32\n",
      "| epoch 122 | iter 10 / 10 | loss 0.32\n",
      "| epoch 123 | iter 10 / 10 | loss 0.31\n",
      "| epoch 124 | iter 10 / 10 | loss 0.31\n",
      "| epoch 125 | iter 10 / 10 | loss 0.30\n",
      "| epoch 126 | iter 10 / 10 | loss 0.30\n",
      "| epoch 127 | iter 10 / 10 | loss 0.28\n",
      "| epoch 128 | iter 10 / 10 | loss 0.28\n",
      "| epoch 129 | iter 10 / 10 | loss 0.28\n",
      "| epoch 130 | iter 10 / 10 | loss 0.28\n",
      "| epoch 131 | iter 10 / 10 | loss 0.27\n",
      "| epoch 132 | iter 10 / 10 | loss 0.27\n",
      "| epoch 133 | iter 10 / 10 | loss 0.27\n",
      "| epoch 134 | iter 10 / 10 | loss 0.27\n",
      "| epoch 135 | iter 10 / 10 | loss 0.27\n",
      "| epoch 136 | iter 10 / 10 | loss 0.26\n",
      "| epoch 137 | iter 10 / 10 | loss 0.26\n",
      "| epoch 138 | iter 10 / 10 | loss 0.26\n",
      "| epoch 139 | iter 10 / 10 | loss 0.25\n",
      "| epoch 140 | iter 10 / 10 | loss 0.24\n",
      "| epoch 141 | iter 10 / 10 | loss 0.24\n",
      "| epoch 142 | iter 10 / 10 | loss 0.25\n",
      "| epoch 143 | iter 10 / 10 | loss 0.24\n",
      "| epoch 144 | iter 10 / 10 | loss 0.24\n",
      "| epoch 145 | iter 10 / 10 | loss 0.23\n",
      "| epoch 146 | iter 10 / 10 | loss 0.24\n",
      "| epoch 147 | iter 10 / 10 | loss 0.23\n",
      "| epoch 148 | iter 10 / 10 | loss 0.23\n",
      "| epoch 149 | iter 10 / 10 | loss 0.22\n",
      "| epoch 150 | iter 10 / 10 | loss 0.22\n",
      "| epoch 151 | iter 10 / 10 | loss 0.22\n",
      "| epoch 152 | iter 10 / 10 | loss 0.22\n",
      "| epoch 153 | iter 10 / 10 | loss 0.22\n",
      "| epoch 154 | iter 10 / 10 | loss 0.22\n",
      "| epoch 155 | iter 10 / 10 | loss 0.22\n",
      "| epoch 156 | iter 10 / 10 | loss 0.21\n",
      "| epoch 157 | iter 10 / 10 | loss 0.21\n",
      "| epoch 158 | iter 10 / 10 | loss 0.20\n",
      "| epoch 159 | iter 10 / 10 | loss 0.21\n",
      "| epoch 160 | iter 10 / 10 | loss 0.20\n",
      "| epoch 161 | iter 10 / 10 | loss 0.20\n",
      "| epoch 162 | iter 10 / 10 | loss 0.20\n",
      "| epoch 163 | iter 10 / 10 | loss 0.21\n",
      "| epoch 164 | iter 10 / 10 | loss 0.20\n",
      "| epoch 165 | iter 10 / 10 | loss 0.20\n",
      "| epoch 166 | iter 10 / 10 | loss 0.19\n",
      "| epoch 167 | iter 10 / 10 | loss 0.19\n",
      "| epoch 168 | iter 10 / 10 | loss 0.19\n",
      "| epoch 169 | iter 10 / 10 | loss 0.19\n",
      "| epoch 170 | iter 10 / 10 | loss 0.19\n",
      "| epoch 171 | iter 10 / 10 | loss 0.19\n",
      "| epoch 172 | iter 10 / 10 | loss 0.18\n",
      "| epoch 173 | iter 10 / 10 | loss 0.18\n",
      "| epoch 174 | iter 10 / 10 | loss 0.18\n",
      "| epoch 175 | iter 10 / 10 | loss 0.18\n",
      "| epoch 176 | iter 10 / 10 | loss 0.18\n",
      "| epoch 177 | iter 10 / 10 | loss 0.18\n",
      "| epoch 178 | iter 10 / 10 | loss 0.18\n",
      "| epoch 179 | iter 10 / 10 | loss 0.17\n",
      "| epoch 180 | iter 10 / 10 | loss 0.17\n",
      "| epoch 181 | iter 10 / 10 | loss 0.18\n",
      "| epoch 182 | iter 10 / 10 | loss 0.17\n",
      "| epoch 183 | iter 10 / 10 | loss 0.18\n",
      "| epoch 184 | iter 10 / 10 | loss 0.17\n",
      "| epoch 185 | iter 10 / 10 | loss 0.17\n",
      "| epoch 186 | iter 10 / 10 | loss 0.18\n",
      "| epoch 187 | iter 10 / 10 | loss 0.17\n",
      "| epoch 188 | iter 10 / 10 | loss 0.17\n",
      "| epoch 189 | iter 10 / 10 | loss 0.17\n",
      "| epoch 190 | iter 10 / 10 | loss 0.17\n",
      "| epoch 191 | iter 10 / 10 | loss 0.16\n",
      "| epoch 192 | iter 10 / 10 | loss 0.17\n",
      "| epoch 193 | iter 10 / 10 | loss 0.16\n",
      "| epoch 194 | iter 10 / 10 | loss 0.16\n",
      "| epoch 195 | iter 10 / 10 | loss 0.16\n",
      "| epoch 196 | iter 10 / 10 | loss 0.16\n",
      "| epoch 197 | iter 10 / 10 | loss 0.16\n",
      "| epoch 198 | iter 10 / 10 | loss 0.15\n",
      "| epoch 199 | iter 10 / 10 | loss 0.16\n",
      "| epoch 200 | iter 10 / 10 | loss 0.16\n",
      "| epoch 201 | iter 10 / 10 | loss 0.15\n",
      "| epoch 202 | iter 10 / 10 | loss 0.16\n",
      "| epoch 203 | iter 10 / 10 | loss 0.16\n",
      "| epoch 204 | iter 10 / 10 | loss 0.15\n",
      "| epoch 205 | iter 10 / 10 | loss 0.16\n",
      "| epoch 206 | iter 10 / 10 | loss 0.15\n",
      "| epoch 207 | iter 10 / 10 | loss 0.15\n",
      "| epoch 208 | iter 10 / 10 | loss 0.15\n",
      "| epoch 209 | iter 10 / 10 | loss 0.15\n",
      "| epoch 210 | iter 10 / 10 | loss 0.15\n",
      "| epoch 211 | iter 10 / 10 | loss 0.15\n",
      "| epoch 212 | iter 10 / 10 | loss 0.15\n",
      "| epoch 213 | iter 10 / 10 | loss 0.15\n",
      "| epoch 214 | iter 10 / 10 | loss 0.15\n",
      "| epoch 215 | iter 10 / 10 | loss 0.15\n",
      "| epoch 216 | iter 10 / 10 | loss 0.14\n",
      "| epoch 217 | iter 10 / 10 | loss 0.14\n",
      "| epoch 218 | iter 10 / 10 | loss 0.15\n",
      "| epoch 219 | iter 10 / 10 | loss 0.14\n",
      "| epoch 220 | iter 10 / 10 | loss 0.14\n",
      "| epoch 221 | iter 10 / 10 | loss 0.14\n",
      "| epoch 222 | iter 10 / 10 | loss 0.14\n",
      "| epoch 223 | iter 10 / 10 | loss 0.14\n",
      "| epoch 224 | iter 10 / 10 | loss 0.14\n",
      "| epoch 225 | iter 10 / 10 | loss 0.14\n",
      "| epoch 226 | iter 10 / 10 | loss 0.14\n",
      "| epoch 227 | iter 10 / 10 | loss 0.14\n",
      "| epoch 228 | iter 10 / 10 | loss 0.14\n",
      "| epoch 229 | iter 10 / 10 | loss 0.13\n",
      "| epoch 230 | iter 10 / 10 | loss 0.14\n",
      "| epoch 231 | iter 10 / 10 | loss 0.13\n",
      "| epoch 232 | iter 10 / 10 | loss 0.14\n",
      "| epoch 233 | iter 10 / 10 | loss 0.13\n",
      "| epoch 234 | iter 10 / 10 | loss 0.13\n",
      "| epoch 235 | iter 10 / 10 | loss 0.13\n",
      "| epoch 236 | iter 10 / 10 | loss 0.13\n",
      "| epoch 237 | iter 10 / 10 | loss 0.14\n",
      "| epoch 238 | iter 10 / 10 | loss 0.13\n",
      "| epoch 239 | iter 10 / 10 | loss 0.13\n",
      "| epoch 240 | iter 10 / 10 | loss 0.14\n",
      "| epoch 241 | iter 10 / 10 | loss 0.13\n",
      "| epoch 242 | iter 10 / 10 | loss 0.13\n",
      "| epoch 243 | iter 10 / 10 | loss 0.13\n",
      "| epoch 244 | iter 10 / 10 | loss 0.13\n",
      "| epoch 245 | iter 10 / 10 | loss 0.13\n",
      "| epoch 246 | iter 10 / 10 | loss 0.13\n",
      "| epoch 247 | iter 10 / 10 | loss 0.13\n",
      "| epoch 248 | iter 10 / 10 | loss 0.13\n",
      "| epoch 249 | iter 10 / 10 | loss 0.13\n",
      "| epoch 250 | iter 10 / 10 | loss 0.13\n",
      "| epoch 251 | iter 10 / 10 | loss 0.13\n",
      "| epoch 252 | iter 10 / 10 | loss 0.12\n",
      "| epoch 253 | iter 10 / 10 | loss 0.12\n",
      "| epoch 254 | iter 10 / 10 | loss 0.12\n",
      "| epoch 255 | iter 10 / 10 | loss 0.12\n",
      "| epoch 256 | iter 10 / 10 | loss 0.12\n",
      "| epoch 257 | iter 10 / 10 | loss 0.12\n",
      "| epoch 258 | iter 10 / 10 | loss 0.12\n",
      "| epoch 259 | iter 10 / 10 | loss 0.13\n",
      "| epoch 260 | iter 10 / 10 | loss 0.12\n",
      "| epoch 261 | iter 10 / 10 | loss 0.13\n",
      "| epoch 262 | iter 10 / 10 | loss 0.12\n",
      "| epoch 263 | iter 10 / 10 | loss 0.12\n",
      "| epoch 264 | iter 10 / 10 | loss 0.13\n",
      "| epoch 265 | iter 10 / 10 | loss 0.12\n",
      "| epoch 266 | iter 10 / 10 | loss 0.12\n",
      "| epoch 267 | iter 10 / 10 | loss 0.12\n",
      "| epoch 268 | iter 10 / 10 | loss 0.12\n",
      "| epoch 269 | iter 10 / 10 | loss 0.11\n",
      "| epoch 270 | iter 10 / 10 | loss 0.12\n",
      "| epoch 271 | iter 10 / 10 | loss 0.12\n",
      "| epoch 272 | iter 10 / 10 | loss 0.12\n",
      "| epoch 273 | iter 10 / 10 | loss 0.12\n",
      "| epoch 274 | iter 10 / 10 | loss 0.12\n",
      "| epoch 275 | iter 10 / 10 | loss 0.11\n",
      "| epoch 276 | iter 10 / 10 | loss 0.12\n",
      "| epoch 277 | iter 10 / 10 | loss 0.12\n",
      "| epoch 278 | iter 10 / 10 | loss 0.11\n",
      "| epoch 279 | iter 10 / 10 | loss 0.11\n",
      "| epoch 280 | iter 10 / 10 | loss 0.11\n",
      "| epoch 281 | iter 10 / 10 | loss 0.11\n",
      "| epoch 282 | iter 10 / 10 | loss 0.12\n",
      "| epoch 283 | iter 10 / 10 | loss 0.11\n",
      "| epoch 284 | iter 10 / 10 | loss 0.11\n",
      "| epoch 285 | iter 10 / 10 | loss 0.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 286 | iter 10 / 10 | loss 0.11\n",
      "| epoch 287 | iter 10 / 10 | loss 0.11\n",
      "| epoch 288 | iter 10 / 10 | loss 0.12\n",
      "| epoch 289 | iter 10 / 10 | loss 0.11\n",
      "| epoch 290 | iter 10 / 10 | loss 0.11\n",
      "| epoch 291 | iter 10 / 10 | loss 0.11\n",
      "| epoch 292 | iter 10 / 10 | loss 0.11\n",
      "| epoch 293 | iter 10 / 10 | loss 0.11\n",
      "| epoch 294 | iter 10 / 10 | loss 0.11\n",
      "| epoch 295 | iter 10 / 10 | loss 0.12\n",
      "| epoch 296 | iter 10 / 10 | loss 0.11\n",
      "| epoch 297 | iter 10 / 10 | loss 0.12\n",
      "| epoch 298 | iter 10 / 10 | loss 0.11\n",
      "| epoch 299 | iter 10 / 10 | loss 0.11\n",
      "| epoch 300 | iter 10 / 10 | loss 0.11\n"
     ]
    }
   ],
   "source": [
    "# 学習用のコードを示す\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.optimizer import SGD\n",
    "from dataset import spiral\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ❶ハイパーパラメータの設定\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "\n",
    "# ②データの読み込み，モデルとオプティマイザの生成\n",
    "x, t = spiral.load_data()\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "# 学習で使用する変数\n",
    "data_size = len(x)\n",
    "max_iters = data_size // batch_size\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # ③データのシャッフル\n",
    "    idx = np.random.permutation(data_size)\n",
    "    x = x[idx]\n",
    "    t = t[idx]\n",
    "    \n",
    "    for iters in range(max_iters):\n",
    "        batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "        batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "        \n",
    "        # ④勾配を求め，パラメータを更新\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        \n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "        \n",
    "        # ⑤定期的に学習経過を出力\n",
    "        if (iters+1) % 10 == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            print('| epoch %d | iter %d / %d | loss %.2f'\n",
    "                 % (epoch + 1, iters + 1, max_iters, avg_loss))\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee6db10",
   "metadata": {},
   "source": [
    "#### エポック（epoch）\n",
    "学習の単位を表す．1エポックは学習データをすべて見たとき，データセットを一蹴したときに相当する．ここでは300エポックの学習を行う．\n",
    "\n",
    "#### 決定境界\n",
    "学習後のニューラルネットワークの分離領域\n",
    "\n",
    "##### 学習後のニューラルネットワークは「渦巻」のパターンを正しくとらえた．非線形や分離領域を学習することができた．ニューラルネットワークは隠れ層を持つことで複雑な表現が可能になる．さらに層を重ねれば，その表現力がより豊かになるのがディープラーニングの特徴．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd6bb5",
   "metadata": {},
   "source": [
    "### 1.4.4 Trainerクラス"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42f8e0",
   "metadata": {},
   "source": [
    "今後，ニューラルネットワークの学習を実行する機会が多くある．学習用のコードを毎回同じように実装するのは退屈なので，学習を行うクラスをTrainerクラスとして提供する．\n",
    "\n",
    "###### Trainerクラスはcommon/trainer.pyにあるので適宜参照．\n",
    "\n",
    "このクラスのイニシャライザは，ニューラルネットワーク（モデル）とオプティマイザを受け取る．\n",
    "\n",
    "学習を開始する際にはfit()メソッドを読んで学習を開始する．\n",
    "\n",
    "#### fit()メソッドの引数\n",
    "- x: 入力データ\n",
    "- t: 教師ラベル\n",
    "- max_epoch(=10): 学習を行うエポック数\n",
    "- batch_size(=32): ミニバッチのサイズ\n",
    "- eval_interval(=20): 結果（平均損失など）を表示するインターバル．例えばeval_interval=20と設定すると，20イテレーションごとに損失の平均を求め，その結果を画面に出力する．\n",
    "- max_grad(=None): 勾配の最大ノルム．勾配のノルムがこの値を超えた場合，勾配を小さくする．（勾配クリッピング）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe78a2",
   "metadata": {},
   "source": [
    "Trainerクラスにはplot()というメソッドがあるので，fit()メソッドで開始し，eval_intervalのタイミングで評価された平均損失をプロットする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e91f05a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 10 | time 0[s] | loss 1.10\n",
      "| epoch 2 |  iter 1 / 10 | time 0[s] | loss 1.12\n",
      "| epoch 3 |  iter 1 / 10 | time 0[s] | loss 1.13\n",
      "| epoch 4 |  iter 1 / 10 | time 0[s] | loss 1.12\n",
      "| epoch 5 |  iter 1 / 10 | time 0[s] | loss 1.12\n",
      "| epoch 6 |  iter 1 / 10 | time 0[s] | loss 1.10\n",
      "| epoch 7 |  iter 1 / 10 | time 0[s] | loss 1.14\n",
      "| epoch 8 |  iter 1 / 10 | time 0[s] | loss 1.16\n",
      "| epoch 9 |  iter 1 / 10 | time 0[s] | loss 1.11\n",
      "| epoch 10 |  iter 1 / 10 | time 0[s] | loss 1.12\n",
      "| epoch 11 |  iter 1 / 10 | time 0[s] | loss 1.12\n",
      "| epoch 12 |  iter 1 / 10 | time 0[s] | loss 1.12\n",
      "| epoch 13 |  iter 1 / 10 | time 0[s] | loss 1.10\n",
      "| epoch 14 |  iter 1 / 10 | time 0[s] | loss 1.09\n",
      "| epoch 15 |  iter 1 / 10 | time 0[s] | loss 1.08\n",
      "| epoch 16 |  iter 1 / 10 | time 0[s] | loss 1.04\n",
      "| epoch 17 |  iter 1 / 10 | time 0[s] | loss 1.03\n",
      "| epoch 18 |  iter 1 / 10 | time 0[s] | loss 0.94\n",
      "| epoch 19 |  iter 1 / 10 | time 0[s] | loss 0.92\n",
      "| epoch 20 |  iter 1 / 10 | time 0[s] | loss 0.92\n",
      "| epoch 21 |  iter 1 / 10 | time 0[s] | loss 0.87\n",
      "| epoch 22 |  iter 1 / 10 | time 0[s] | loss 0.85\n",
      "| epoch 23 |  iter 1 / 10 | time 0[s] | loss 0.80\n",
      "| epoch 24 |  iter 1 / 10 | time 0[s] | loss 0.79\n",
      "| epoch 25 |  iter 1 / 10 | time 0[s] | loss 0.78\n",
      "| epoch 26 |  iter 1 / 10 | time 0[s] | loss 0.83\n",
      "| epoch 27 |  iter 1 / 10 | time 0[s] | loss 0.77\n",
      "| epoch 28 |  iter 1 / 10 | time 0[s] | loss 0.76\n",
      "| epoch 29 |  iter 1 / 10 | time 0[s] | loss 0.77\n",
      "| epoch 30 |  iter 1 / 10 | time 0[s] | loss 0.76\n",
      "| epoch 31 |  iter 1 / 10 | time 0[s] | loss 0.77\n",
      "| epoch 32 |  iter 1 / 10 | time 0[s] | loss 0.75\n",
      "| epoch 33 |  iter 1 / 10 | time 0[s] | loss 0.78\n",
      "| epoch 34 |  iter 1 / 10 | time 0[s] | loss 0.77\n",
      "| epoch 35 |  iter 1 / 10 | time 0[s] | loss 0.78\n",
      "| epoch 36 |  iter 1 / 10 | time 0[s] | loss 0.74\n",
      "| epoch 37 |  iter 1 / 10 | time 0[s] | loss 0.75\n",
      "| epoch 38 |  iter 1 / 10 | time 0[s] | loss 0.77\n",
      "| epoch 39 |  iter 1 / 10 | time 0[s] | loss 0.75\n",
      "| epoch 40 |  iter 1 / 10 | time 0[s] | loss 0.73\n",
      "| epoch 41 |  iter 1 / 10 | time 0[s] | loss 0.75\n",
      "| epoch 42 |  iter 1 / 10 | time 0[s] | loss 0.76\n",
      "| epoch 43 |  iter 1 / 10 | time 0[s] | loss 0.79\n",
      "| epoch 44 |  iter 1 / 10 | time 0[s] | loss 0.74\n",
      "| epoch 45 |  iter 1 / 10 | time 0[s] | loss 0.75\n",
      "| epoch 46 |  iter 1 / 10 | time 0[s] | loss 0.73\n",
      "| epoch 47 |  iter 1 / 10 | time 0[s] | loss 0.73\n",
      "| epoch 48 |  iter 1 / 10 | time 0[s] | loss 0.73\n",
      "| epoch 49 |  iter 1 / 10 | time 0[s] | loss 0.73\n",
      "| epoch 50 |  iter 1 / 10 | time 0[s] | loss 0.72\n",
      "| epoch 51 |  iter 1 / 10 | time 0[s] | loss 0.72\n",
      "| epoch 52 |  iter 1 / 10 | time 0[s] | loss 0.72\n",
      "| epoch 53 |  iter 1 / 10 | time 0[s] | loss 0.72\n",
      "| epoch 54 |  iter 1 / 10 | time 0[s] | loss 0.74\n",
      "| epoch 55 |  iter 1 / 10 | time 0[s] | loss 0.74\n",
      "| epoch 56 |  iter 1 / 10 | time 0[s] | loss 0.73\n",
      "| epoch 57 |  iter 1 / 10 | time 0[s] | loss 0.72\n",
      "| epoch 58 |  iter 1 / 10 | time 0[s] | loss 0.69\n",
      "| epoch 59 |  iter 1 / 10 | time 0[s] | loss 0.72\n",
      "| epoch 60 |  iter 1 / 10 | time 0[s] | loss 0.70\n",
      "| epoch 61 |  iter 1 / 10 | time 0[s] | loss 0.69\n",
      "| epoch 62 |  iter 1 / 10 | time 0[s] | loss 0.71\n",
      "| epoch 63 |  iter 1 / 10 | time 0[s] | loss 0.70\n",
      "| epoch 64 |  iter 1 / 10 | time 0[s] | loss 0.71\n",
      "| epoch 65 |  iter 1 / 10 | time 0[s] | loss 0.72\n",
      "| epoch 66 |  iter 1 / 10 | time 0[s] | loss 0.71\n",
      "| epoch 67 |  iter 1 / 10 | time 0[s] | loss 0.71\n",
      "| epoch 68 |  iter 1 / 10 | time 0[s] | loss 0.71\n",
      "| epoch 69 |  iter 1 / 10 | time 0[s] | loss 0.70\n",
      "| epoch 70 |  iter 1 / 10 | time 0[s] | loss 0.68\n",
      "| epoch 71 |  iter 1 / 10 | time 0[s] | loss 0.73\n",
      "| epoch 72 |  iter 1 / 10 | time 0[s] | loss 0.66\n",
      "| epoch 73 |  iter 1 / 10 | time 0[s] | loss 0.69\n",
      "| epoch 74 |  iter 1 / 10 | time 0[s] | loss 0.66\n",
      "| epoch 75 |  iter 1 / 10 | time 0[s] | loss 0.70\n",
      "| epoch 76 |  iter 1 / 10 | time 0[s] | loss 0.65\n",
      "| epoch 77 |  iter 1 / 10 | time 0[s] | loss 0.67\n",
      "| epoch 78 |  iter 1 / 10 | time 0[s] | loss 0.70\n",
      "| epoch 79 |  iter 1 / 10 | time 0[s] | loss 0.63\n",
      "| epoch 80 |  iter 1 / 10 | time 0[s] | loss 0.66\n",
      "| epoch 81 |  iter 1 / 10 | time 0[s] | loss 0.65\n",
      "| epoch 82 |  iter 1 / 10 | time 0[s] | loss 0.66\n",
      "| epoch 83 |  iter 1 / 10 | time 0[s] | loss 0.64\n",
      "| epoch 84 |  iter 1 / 10 | time 0[s] | loss 0.62\n",
      "| epoch 85 |  iter 1 / 10 | time 0[s] | loss 0.62\n",
      "| epoch 86 |  iter 1 / 10 | time 0[s] | loss 0.63\n",
      "| epoch 87 |  iter 1 / 10 | time 0[s] | loss 0.59\n",
      "| epoch 88 |  iter 1 / 10 | time 0[s] | loss 0.58\n",
      "| epoch 89 |  iter 1 / 10 | time 0[s] | loss 0.61\n",
      "| epoch 90 |  iter 1 / 10 | time 0[s] | loss 0.59\n",
      "| epoch 91 |  iter 1 / 10 | time 0[s] | loss 0.58\n",
      "| epoch 92 |  iter 1 / 10 | time 0[s] | loss 0.57\n",
      "| epoch 93 |  iter 1 / 10 | time 0[s] | loss 0.55\n",
      "| epoch 94 |  iter 1 / 10 | time 0[s] | loss 0.54\n",
      "| epoch 95 |  iter 1 / 10 | time 0[s] | loss 0.53\n",
      "| epoch 96 |  iter 1 / 10 | time 0[s] | loss 0.54\n",
      "| epoch 97 |  iter 1 / 10 | time 0[s] | loss 0.51\n",
      "| epoch 98 |  iter 1 / 10 | time 0[s] | loss 0.51\n",
      "| epoch 99 |  iter 1 / 10 | time 0[s] | loss 0.50\n",
      "| epoch 100 |  iter 1 / 10 | time 0[s] | loss 0.47\n",
      "| epoch 101 |  iter 1 / 10 | time 0[s] | loss 0.49\n",
      "| epoch 102 |  iter 1 / 10 | time 0[s] | loss 0.46\n",
      "| epoch 103 |  iter 1 / 10 | time 0[s] | loss 0.44\n",
      "| epoch 104 |  iter 1 / 10 | time 0[s] | loss 0.47\n",
      "| epoch 105 |  iter 1 / 10 | time 0[s] | loss 0.44\n",
      "| epoch 106 |  iter 1 / 10 | time 0[s] | loss 0.43\n",
      "| epoch 107 |  iter 1 / 10 | time 0[s] | loss 0.43\n",
      "| epoch 108 |  iter 1 / 10 | time 0[s] | loss 0.39\n",
      "| epoch 109 |  iter 1 / 10 | time 0[s] | loss 0.40\n",
      "| epoch 110 |  iter 1 / 10 | time 0[s] | loss 0.41\n",
      "| epoch 111 |  iter 1 / 10 | time 0[s] | loss 0.38\n",
      "| epoch 112 |  iter 1 / 10 | time 0[s] | loss 0.38\n",
      "| epoch 113 |  iter 1 / 10 | time 0[s] | loss 0.38\n",
      "| epoch 114 |  iter 1 / 10 | time 0[s] | loss 0.37\n",
      "| epoch 115 |  iter 1 / 10 | time 0[s] | loss 0.36\n",
      "| epoch 116 |  iter 1 / 10 | time 0[s] | loss 0.34\n",
      "| epoch 117 |  iter 1 / 10 | time 0[s] | loss 0.35\n",
      "| epoch 118 |  iter 1 / 10 | time 0[s] | loss 0.33\n",
      "| epoch 119 |  iter 1 / 10 | time 0[s] | loss 0.35\n",
      "| epoch 120 |  iter 1 / 10 | time 0[s] | loss 0.33\n",
      "| epoch 121 |  iter 1 / 10 | time 0[s] | loss 0.33\n",
      "| epoch 122 |  iter 1 / 10 | time 0[s] | loss 0.32\n",
      "| epoch 123 |  iter 1 / 10 | time 0[s] | loss 0.31\n",
      "| epoch 124 |  iter 1 / 10 | time 0[s] | loss 0.31\n",
      "| epoch 125 |  iter 1 / 10 | time 0[s] | loss 0.31\n",
      "| epoch 126 |  iter 1 / 10 | time 0[s] | loss 0.30\n",
      "| epoch 127 |  iter 1 / 10 | time 0[s] | loss 0.30\n",
      "| epoch 128 |  iter 1 / 10 | time 0[s] | loss 0.27\n",
      "| epoch 129 |  iter 1 / 10 | time 0[s] | loss 0.30\n",
      "| epoch 130 |  iter 1 / 10 | time 0[s] | loss 0.28\n",
      "| epoch 131 |  iter 1 / 10 | time 0[s] | loss 0.26\n",
      "| epoch 132 |  iter 1 / 10 | time 0[s] | loss 0.27\n",
      "| epoch 133 |  iter 1 / 10 | time 0[s] | loss 0.27\n",
      "| epoch 134 |  iter 1 / 10 | time 0[s] | loss 0.28\n",
      "| epoch 135 |  iter 1 / 10 | time 0[s] | loss 0.26\n",
      "| epoch 136 |  iter 1 / 10 | time 0[s] | loss 0.28\n",
      "| epoch 137 |  iter 1 / 10 | time 0[s] | loss 0.25\n",
      "| epoch 138 |  iter 1 / 10 | time 0[s] | loss 0.26\n",
      "| epoch 139 |  iter 1 / 10 | time 0[s] | loss 0.26\n",
      "| epoch 140 |  iter 1 / 10 | time 0[s] | loss 0.26\n",
      "| epoch 141 |  iter 1 / 10 | time 0[s] | loss 0.23\n",
      "| epoch 142 |  iter 1 / 10 | time 0[s] | loss 0.23\n",
      "| epoch 143 |  iter 1 / 10 | time 0[s] | loss 0.26\n",
      "| epoch 144 |  iter 1 / 10 | time 0[s] | loss 0.23\n",
      "| epoch 145 |  iter 1 / 10 | time 0[s] | loss 0.24\n",
      "| epoch 146 |  iter 1 / 10 | time 0[s] | loss 0.24\n",
      "| epoch 147 |  iter 1 / 10 | time 0[s] | loss 0.25\n",
      "| epoch 148 |  iter 1 / 10 | time 0[s] | loss 0.21\n",
      "| epoch 149 |  iter 1 / 10 | time 0[s] | loss 0.23\n",
      "| epoch 150 |  iter 1 / 10 | time 0[s] | loss 0.22\n",
      "| epoch 151 |  iter 1 / 10 | time 0[s] | loss 0.22\n",
      "| epoch 152 |  iter 1 / 10 | time 0[s] | loss 0.23\n",
      "| epoch 153 |  iter 1 / 10 | time 0[s] | loss 0.23\n",
      "| epoch 154 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
      "| epoch 155 |  iter 1 / 10 | time 0[s] | loss 0.22\n",
      "| epoch 156 |  iter 1 / 10 | time 0[s] | loss 0.21\n",
      "| epoch 157 |  iter 1 / 10 | time 0[s] | loss 0.21\n",
      "| epoch 158 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
      "| epoch 159 |  iter 1 / 10 | time 0[s] | loss 0.21\n",
      "| epoch 160 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
      "| epoch 161 |  iter 1 / 10 | time 0[s] | loss 0.19\n",
      "| epoch 162 |  iter 1 / 10 | time 0[s] | loss 0.22\n",
      "| epoch 163 |  iter 1 / 10 | time 0[s] | loss 0.19\n",
      "| epoch 164 |  iter 1 / 10 | time 0[s] | loss 0.21\n",
      "| epoch 165 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
      "| epoch 166 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
      "| epoch 167 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
      "| epoch 168 |  iter 1 / 10 | time 0[s] | loss 0.19\n",
      "| epoch 169 |  iter 1 / 10 | time 0[s] | loss 0.18\n",
      "| epoch 170 |  iter 1 / 10 | time 0[s] | loss 0.19\n",
      "| epoch 171 |  iter 1 / 10 | time 0[s] | loss 0.19\n",
      "| epoch 172 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
      "| epoch 173 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 174 |  iter 1 / 10 | time 0[s] | loss 0.20\n",
      "| epoch 175 |  iter 1 / 10 | time 0[s] | loss 0.18\n",
      "| epoch 176 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
      "| epoch 177 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
      "| epoch 178 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
      "| epoch 179 |  iter 1 / 10 | time 0[s] | loss 0.18\n",
      "| epoch 180 |  iter 1 / 10 | time 0[s] | loss 0.19\n",
      "| epoch 181 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
      "| epoch 182 |  iter 1 / 10 | time 0[s] | loss 0.18\n",
      "| epoch 183 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 184 |  iter 1 / 10 | time 0[s] | loss 0.18\n",
      "| epoch 185 |  iter 1 / 10 | time 0[s] | loss 0.18\n",
      "| epoch 186 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
      "| epoch 187 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
      "| epoch 188 |  iter 1 / 10 | time 0[s] | loss 0.18\n",
      "| epoch 189 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 190 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 191 |  iter 1 / 10 | time 0[s] | loss 0.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 192 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
      "| epoch 193 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 194 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 195 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 196 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
      "| epoch 197 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 198 |  iter 1 / 10 | time 0[s] | loss 0.17\n",
      "| epoch 199 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 200 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 201 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 202 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 203 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
      "| epoch 204 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 205 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 206 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 207 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 208 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 209 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
      "| epoch 210 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 211 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 212 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
      "| epoch 213 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
      "| epoch 214 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
      "| epoch 215 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 216 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 217 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
      "| epoch 218 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 219 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
      "| epoch 220 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
      "| epoch 221 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 222 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 223 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
      "| epoch 224 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 225 |  iter 1 / 10 | time 0[s] | loss 0.16\n",
      "| epoch 226 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 227 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 228 |  iter 1 / 10 | time 0[s] | loss 0.15\n",
      "| epoch 229 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 230 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 231 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 232 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 233 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 234 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 235 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 236 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 237 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 238 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 239 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 240 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 241 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 242 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 243 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 244 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 245 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 246 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 247 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 248 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 249 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 250 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 251 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 252 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 253 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 254 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 255 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 256 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 257 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 258 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 259 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 260 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 261 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 262 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 263 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 264 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 265 |  iter 1 / 10 | time 0[s] | loss 0.14\n",
      "| epoch 266 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 267 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 268 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 269 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 270 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 271 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 272 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 273 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 274 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 275 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 276 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 277 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 278 |  iter 1 / 10 | time 0[s] | loss 0.13\n",
      "| epoch 279 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 280 |  iter 1 / 10 | time 0[s] | loss 0.10\n",
      "| epoch 281 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 282 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 283 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 284 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 285 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 286 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 287 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 288 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 289 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 290 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 291 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 292 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 293 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 294 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 295 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 296 |  iter 1 / 10 | time 0[s] | loss 0.12\n",
      "| epoch 297 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 298 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 299 |  iter 1 / 10 | time 0[s] | loss 0.11\n",
      "| epoch 300 |  iter 1 / 10 | time 0[s] | loss 0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['IPAexGothic'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trainerクラスを使って学習を行うコード\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import Trainer\n",
    "from dataset import spiral\n",
    "\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "\n",
    "x, t = spiral.load_data()\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "oprimizer = SGD(lr=learning_rate)\n",
    "\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.fit(x, t, max_epoch, batch_size, eval_interval=10)\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285bedd",
   "metadata": {},
   "source": [
    "以前と同じようにニューラルネットワークの学習が行われた．以前に示した学習用コードをTrainerクラスに担わせることで，コードがすっきりした．今後はTrainerクラスを使って学習を行う．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0c9aa2",
   "metadata": {},
   "source": [
    "## 1.5 計算の高速化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b91bbb",
   "metadata": {},
   "source": [
    "ニューラルネットワークをいかに高速に計算するかということは重要なテーマにになる．ここではニューラルネットワークの高速化に有効な\n",
    "- ビット精度\n",
    "- GPU\n",
    "\n",
    "について説明する．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e43523",
   "metadata": {},
   "source": [
    "### 1.5.1 ビット精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e45585c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 64ビット浮動小数点数が使われているか調べる\n",
    "import numpy as np\n",
    "a = np.random.randn(3)\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af91790",
   "metadata": {},
   "source": [
    "ニューラルネットワークの推論及び学習は，32ビットの浮動小数点数で問題なく，認識精度を落とすことなく，行えることが知られている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dec4a2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# NumPyで32ビット浮動小数点を優先して利用する\n",
    "# 2通りの指定で確認\n",
    "b = np.random.randn(3).astype(np.float32)\n",
    "print(b.dtype)\n",
    "\n",
    "c = np.random.randn(3).astype('f')\n",
    "print(c.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b96509",
   "metadata": {},
   "source": [
    "NumPyは16ビットの浮動小数点数が用意されるが，一般的なCPUやGPUでは演算が32ビットで行われるため，処理速度の向上は見込めない．一方，学習した重みを（外部ファイル）に保存するようなケースでは容量を半分しか食わずに済むので有効．\n",
    "\n",
    "今後は，学習した重みを保存するときに限り，16ビットの浮動小数点数に変換することにする．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f427e99",
   "metadata": {},
   "source": [
    "ディープラーニングの注目に沿って，最近のGPUでは16ビットの半精度浮動小数点数がストレージと演算の両方でサポートされている．GoogleのTPUと呼ばれる独自チップは8ビットで計算できるように工夫されている．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b623f4",
   "metadata": {},
   "source": [
    "### 1.5.2 GPU(CuPy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae5720a",
   "metadata": {},
   "source": [
    "大量の積和演算の多くは並列計算が可能であり，CPUよりもGPUの方が適している．一般的なディープラーニングのフレームワークでは，CPUに加えてGPUでも実行できるように設計されている．\n",
    "\n",
    "CuPy（Pythonライブラリ）を使用する．CuPyを利用するには，NVIDIA製のGPUを備えたマシンが必要．また，CUDAと呼ばれるGPU向けの汎用並列コンピューティング・プラットフォームをインストールする必要がある．\n",
    "\n",
    "CuPyはNumPyと共通のAPIを持つので，基本的に同じ使い方ができる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac7c1e",
   "metadata": {},
   "source": [
    "#### GPUを自分のPCで使いたい\n",
    "GPU計算を行うためにCupyを使おうとしたが、GPUについていろいろ調べたところ、PCに搭載されているGPUがIntel製のもので、NVIDIA製のものではなく環境構築がうまくいかなかった。とりあえず現時点ではGPU実装は見送り、改めて自分のPCにGPU環境を構築するための調べ物をすることにする"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0f248c",
   "metadata": {},
   "source": [
    "## 1.6 まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b935e5fc",
   "metadata": {},
   "source": [
    "#### 本章で学んだこと\n",
    "- ニューラルネットワークは、入力層、隠れ層、出力層を持つ\n",
    "- 全結合層によって線形な変換が行われ、活性化関数によって非線形な変換が行われる\n",
    "- 全結合像やミニバッチ処理は、行列としてまとめて計算することができる\n",
    "- 誤差逆伝播法を使って効率的にニューラルネットワークの損失に関する勾配を求めることができる\n",
    "- ニューラルネットワークで行う処理は，計算グラフによって可視化することができ，順伝播や逆伝播の理解に役立つ\n",
    "- ニューラルネットワークの実装では，構成要素を「レイヤ」としてモジュール化することで，組み立てが容易になる\n",
    "- ニューラルネットワークの高速化において，データのビット精度とGPUによる並列計算が重要である"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db337629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
